#preprocess.py - Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n Ti·∫øng Vi·ªát

# Phase 1: Imports
# TODO: import re, pyvi, sklearn
import re
from pyvi import ViTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer


# Phase 2: Class TextPreprocessor (Singleton)
# TODO: T·∫°o class v·ªõi __new__ v√† _initialize

class TextPreprocessor:
    _instance = None
    def __new__ (cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize()
        return cls._instance
        
    def _initialize(self):
        self.re_special_chars = re.compile(r'[^\w\s]')
        self.re_numbers = re.compile(r'\d+')

        self._load_dictionaries()
        ##L√†m ƒë·ªÉ kh√¥ng ph·∫£i duy·ªát l·∫°i t·ª´ ƒë·∫ßu m·ªói l·∫ßn t√¨m ki·∫øm t·ª´ so s√°nh
        all_phrases = [p for p in self.SYNONYMS.keys() if " " in p]
        all_phrases.sort(key=len, reverse=True)

        if all_phrases:
            pattern = r'\b(' + '|'.join(map(re.escape, all_phrases)) + r')\b'
            self.re_synonym_phrases = re.compile(pattern, re.IGNORECASE)
        else:
            self.re_synonym_phrases = None

    def _load_dictionaries(self):
        """ƒê·ªãnh nghƒ©a c√°c t·ª´ ƒëi·ªÉn d·ªØ li·ªáu."""
        self.VIETNAMESE_STOPWORDS = {
            'th√¨', 'l√†', 'm√†', 'v√†', 'c·ªßa', 'nh·ªØng', 'c√°c', 'nh∆∞', 'th·∫ø', 'n√†o', 
            'ƒë∆∞·ª£c', 'v·ªÅ', 'v·ªõi', 'trong', 'c√≥', 'kh√¥ng', 'cho', 't√¥i', 'b·∫°n', 
            'c·∫≠u', 't·ªõ', 'm√¨nh', 'n√≥', 'h·∫Øn', 'g√¨', 'c√°i', 'con', 'ng∆∞·ªùi', 
            's·ª±', 'vi·ªác', 'ƒë√≥', 'ƒë√¢y', 'kia', 'n√†y', 'nh√©', '·∫°', '∆°i', 'ƒëi', 
            'l√†m', 'khi', 'l√∫c', 'n∆°i', 't·∫°i', 'ƒë√£', 'ƒëang', 's·∫Ω', 'mu·ªën', 
            'ph·∫£i', 'bi·∫øt', 'h√£y', 'r·ªìi', 'ch·ª©', 'nh·ªâ'
        }
        
        # üÜï T·ª´ kh√≥a quan tr·ªçng KH√îNG ƒë∆∞·ª£c x√≥a khi preprocessing cho KNN
        # S·ª≠ d·ª•ng cho semantic matching - c·∫ßn gi·ªØ context
        self.CRITICAL_KEYWORDS = {
            # Thu·∫≠t to√°n AI/ML
            'knn', 'bfs', 'dfs', 'svm', 'cnn', 'rnn', 'lstm', 'transformer',
            'naive', 'bayes', 'decision', 'tree', 'random', 'forest',
            'gradient', 'descent', 'backpropagation', 'softmax', 'sigmoid',
            # Search algorithms
            'minimax', 'alpha', 'beta', 'heuristic', 'admissible', 'consistent',
            'ucs', 'ids', 'a*', 'greedy',
            # Logic
            'modus', 'ponens', 'resolution', 'cnf', 'fol', 'kb',
            # T·ª´ kh√≥a h·ªèi ƒë√°p quan tr·ªçng (gi·ªØ cho KNN)
            'l√†', 'g√¨', 'kh√°c', 'gi·ªëng', 'so', 's√°nh', 't·∫°i', 'sao', 'nh∆∞', 'n√†o',
            # Topics
            'agent', 't√°c', 't·ª≠', 'm√¥i', 'tr∆∞·ªùng', 'h·ªçc', 'm√°y', 's√¢u'
        }
        
        # Stopwords nh·∫π cho KNN - ch·ªâ x√≥a c√°c t·ª´ th·ª±c s·ª± l√† noise
        self.LIGHT_STOPWORDS = {
            'th√¨', 'm√†', 'v√†', 'c·ªßa', 'nh·ªØng', 'c√°c', 'ƒë∆∞·ª£c', 'cho', 't√¥i', 'b·∫°n',
            'c·∫≠u', 't·ªõ', 'm√¨nh', 'n√≥', 'h·∫Øn', 'c√°i', 'con', 's·ª±', 'vi·ªác',
            'ƒë√≥', 'ƒë√¢y', 'kia', 'n√†y', 'nh√©', '·∫°', '∆°i', 'ƒëi', 'r·ªìi', 'ch·ª©', 'nh·ªâ'
        }

        self.SYNONYMS = {
            # 1. Thu·∫≠t to√°n & Kh√°i ni·ªám c∆° b·∫£n
            "knn": ["k-nearest neighbors", "k nearest neighbors", "l√¢n c·∫≠n g·∫ßn nh·∫•t", "k l√¢n c·∫≠n"],
            "naive bayes": ["na√Øve bayes", "bayes ng√¢y th∆°", "bayes"],
            "bfs": ["breadth-first search", "t√¨m ki·∫øm theo chi·ªÅu r·ªông", "chi·ªÅu r·ªông"],
            "dfs": ["depth-first search", "t√¨m ki·∫øm theo chi·ªÅu s√¢u", "chi·ªÅu s√¢u"],
            "a*": ["a star", "a sao", "thu·∫≠t to√°n a*"],
            
            # 2. Logic & Suy di·ªÖn
            "logic m·ªánh ƒë·ªÅ": ["propositional logic", "logic ph√°t bi·ªÉu"],
            "logic v·ªã t·ª´": ["first-order logic", "fol", "logic b·∫≠c nh·∫•t"],
            "kb": ["knowledge base", "c∆° s·ªü tri th·ª©c"],
            
            # 3. H·ªçc m√°y (Machine Learning)
            "h·ªçc c√≥ gi√°m s√°t": ["supervised learning", "h·ªçc gi√°m s√°t"],
            "h·ªçc kh√¥ng gi√°m s√°t": ["unsupervised learning", "h·ªçc kh√¥ng gi√°m s√°t"],
            "h·ªçc tƒÉng c∆∞·ªùng": ["reinforcement learning", "rl"],
            "h·ªçc m√°y": ["machine learning", "ml"],
            "tr√≠ tu·ªá nh√¢n t·∫°o": ["ai", "artificial intelligence"],
            "x·ª≠ l√Ω ng√¥n ng·ªØ": ["nlp", "natural language processing"],
            
            # 4. T√°c t·ª≠ & M√¥i tr∆∞·ªùng
            "t√°c t·ª≠": ["agent", "ƒë·∫°i l√Ω"],
            "peas": ["performance environment actuators sensors", "ƒë·ªô ƒëo m√¥i tr∆∞·ªùng b·ªô ch·∫•p h√†nh c·∫£m bi·∫øn"],
            "m√¥i tr∆∞·ªùng": ["environment"],
            "c·∫£m bi·∫øn": ["sensors"],
            "b·ªô ch·∫•p h√†nh": ["actuators"],

            # 6. T·ª´ kh√≥a h·ªèi ƒë√°p th√¥ng d·ª•ng
            "l√† g√¨": ["l√† c√°i g√¨", "nghƒ©a l√† g√¨", "ƒë·ªãnh nghƒ©a", "kh√°i ni·ªám", "ch·ª©c nƒÉng", "t√°c d·ª•ng", "c√¥ng d·ª•ng", "vai tr√≤", "√Ω nghƒ©a", "d√πng ƒë·ªÉ l√†m g√¨"],
            "t·∫°i sao": ["v√¨ sao", "l√Ω do", "nguy√™n nh√¢n"],
            "nh∆∞ th·∫ø n√†o": ["ra sao", "l√†m sao", "c√°ch n√†o"],
            
            # 5. Thi·∫øt b·ªã & ƒê·ªùi s·ªëng
            "xe h∆°i": ["√¥ t√¥", "x·∫ø h·ªôp", "b·ªën b√°nh"],
            "ƒëi·ªán tho·∫°i": ["d·∫ø", "smartphone", "mobile", "di ƒë·ªông"],
            "m√°y t√≠nh": ["laptop", "pc", "computer", "desktop"],
            "k√©m": ["t·ªá", "x·∫•u", "y·∫øu", "d·ªü"],
            "t·ªët": ["ngon", "x·ªãn", "ƒë·ªânh", "tuy·ªát", "hay"]
        }

        self.WEIGHTED_KEYWORDS = {
            "gi√°": 2.0, "mua": 1.5, "b√°n": 1.5, "l·ªói": 2.0,
            "kh√¥ng": 1.5, "t·∫°i sao": 1.5, "l√† g√¨": 1.2,
            # üî• T·ª´ kh√≥a so s√°nh - ∆∞u ti√™n cao ƒë·ªÉ nh·∫≠n di·ªán c√¢u h·ªèi so s√°nh
            "kh√°c": 3.0, "kh√°c g√¨": 3.0, "kh√°c nhau": 3.0, 
            "so s√°nh": 3.0, "so v·ªõi": 2.5, "gi·ªëng": 2.5,
            "kh√°c bi·ªát": 3.0, "ƒëi·ªÉm kh√°c": 3.0
        }

        self.NEGATION_WORDS = {"kh√¥ng", "ch·∫≥ng", "ch·∫£", "ƒë·ª´ng", "ch∆∞a", "k√©m", "ƒë√¢u"}

        self.REVERSE_SYNONYMS = {}
        ## T√åm m·∫•y c√°i t·ª´ REVERSE truy ng∆∞·ª£c l·∫°i key
        for canonical, variations in self.SYNONYMS.items():
            for var in variations:
                self.REVERSE_SYNONYMS[var] = canonical
            self.REVERSE_SYNONYMS[canonical] = canonical

    def preprocess_text(self, text: str) -> str:
        if not text: return ""

        text = text.lower()
        text = self.re_special_chars.sub('', text)
        text = self.re_numbers.sub("", text)
        
        text_tokenized = ViTokenizer.tokenize(text)

        tokens = text_tokenized.split()

        filtered = [token for token in tokens if token not in self.VIETNAMESE_STOPWORDS and len(token) > 1]
    
        return " ".join(filtered)

    def preprocess_knn(self, text: str) -> str:
        if not text: return ""

        text = text.lower()
        text = self.re_special_chars.sub("", text)

        tokenized_text = ViTokenizer.tokenize(text)
        
        tokens = tokenized_text.split()

        filtered = [token for token in tokens if token in self.CRITICAL_KEYWORDS or (token not in self.LIGHT_STOPWORDS and len(token) > 1)]

        expended_text = self.expand_query(" ".join(filtered))
        return expended_text

    def expand_query(self, query) -> str:
        if not query: return ""

        text = query.lower()
        expanded_tokens = []
        words = text.split()

        for word in words:
            expanded_tokens.append(word)
            if word in self.SYNONYMS:
                expanded_tokens.extend(self.SYNONYMS[word])
        
        
        if self.re_synonym_phrases:
            matches = self.re_synonym_phrases.findall(text)
            for match in matches:
                if match.lower() in self.SYNONYMS:
                    expanded_tokens.extend(self.SYNONYMS[match.lower()])
                    
        return " ".join(expanded_tokens)
            
    def detect_negation(self, text: str) -> str:
        if not text: return ""

        tokens = text.split()
        negation = False
        processed = []

        for token in tokens:
            if token.lower() in self.NEGATION_WORDS:
                negation = True
                processed.append(token)
            elif negation:
                processed.append(f"NOT_{token}")
            else:
                processed.append(token)
            
        return " ".join(processed)

    def weighted_keywords(self, text: str) -> float:

        if not text : return 0.0
        score = 0.0
        text_lower = text.lower()

        for kw, weight in self.WEIGHTED_KEYWORDS.items():
            if kw in text_lower:
                score += weight
        return score
        
    def canonicalize_text(self, text: str) -> set:
        if not text: return ""

        text_lower = text.lower()
        canonical_tokens = set()

        if self.re_synonym_phrases:
            matches = self.re_synonym_phrases.findall(text_lower)
            for match in matches:
                if match.lower() in self.REVERSE_SYNONYMS:
                    canonical_tokens.add(self.REVERSE_SYNONYMS[match.lower()])
        
        for token in text_lower.split():
            if token in self.REVERSE_SYNONYMS:
                canonical_tokens.add(self.REVERSE_SYNONYMS[token])

        return canonical_tokens



    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        if not text1 or not text2: return 0.0

        set1 = self.canonicalize_text(text1)
        set2 = self.canonicalize_text(text2)

        if not set1 and not set2: return 0.0

        intersection = set1.intersection(set2)
        union = set1.union(set2)

        return len(intersection) / len(union) if union else 0.0
# ============================================
# Phase 6: Module-Level Interface
# ============================================
preprocessor = TextPreprocessor()

def preprocess_text(text): return preprocessor.preprocess_text(text)
def preprocess_knn(text): return preprocessor.preprocess_knn(text)
def expand_query(text): return preprocessor.expand_query(text)
def detect_negation(text): return preprocessor.detect_negation(text)
def weighted_keywords(text): return preprocessor.weighted_keywords(text)
def calculate_jaccard_similarity(text1, text2): return preprocessor.calculate_jaccard_similarity(text1, text2)
def canonicalize_text(text): return preprocessor.canonicalize_text(text)

def train_vectorizer(corpus):
    vectorizer = TfidfVectorizer(max_features=800, ngram_range=(1, 2))
    vectorizer.fit(corpus)
    return vectorizer

# ============================================
# Phase 7: Sanity Check
# ============================================
if __name__ == "__main__":
    print("üß™ Testing preprocess.py...\n")
    
    # Test 1: preprocess_text (Naive Bayes)
    test1 = "H·ªçc m√°y l√† g√¨ v·∫≠y b·∫°n?"
    result1 = preprocess_text(test1)
    print(f"‚úÖ preprocess_text:")
    print(f"   Input:  '{test1}'")
    print(f"   Output: '{result1}'\n")
    
    # Test 2: preprocess_knn
    test2 = "AI v√† KNN kh√°c nhau nh∆∞ th·∫ø n√†o?"
    result2 = preprocess_knn(test2)
    print(f"‚úÖ preprocess_knn:")
    print(f"   Input:  '{test2}'")
    print(f"   Output: '{result2}'\n")
    
    # Test 3: expand_query
    test3 = "tr√≠ tu·ªá nh√¢n t·∫°o"
    result3 = expand_query(test3)
    print(f"‚úÖ expand_query:")
    print(f"   Input:  '{test3}'")
    print(f"   Output: '{result3}'\n")
    
    # Test 4: detect_negation
    test4 = "H·ªçc m√°y kh√¥ng t·ªët"
    result4 = detect_negation(test4)
    print(f"‚úÖ detect_negation:")
    print(f"   Input:  '{test4}'")
    print(f"   Output: '{result4}'\n")
    
    # Test 5: weighted_keyword_match
    test5 = "T·∫°i sao KNN l·∫°i kh√°c Naive Bayes?"
    result5 = preprocessor.weighted_keywords(test5)
    print(f"‚úÖ weighted_keyword_match:")
    print(f"   Input:  '{test5}'")
    print(f"   Score: {result5}\n")
    
    # Test 6: calculate_jaccard_similarity
    text_a = "H·ªçc m√°y l√† g√¨"
    text_b = "Machine learning l√† c√°i g√¨"
    result6 = preprocessor.calculate_jaccard_similarity(text_a, text_b)
    print(f"‚úÖ calculate_jaccard_similarity:")
    print(f"   Text A: '{text_a}'")
    print(f"   Text B: '{text_b}'")
    print(f"   Similarity: {result6:.2f}\n")

    text6 = "ml v√† machine learning l√† m·ªôt v√† l√† m·ªôt nh√°nh c·ªßa AI v√† ai v√† tr√≠ tu·ªá nh√¢n t·∫°o"
    result6 = canonicalize_text(text6)
    print(f"‚úÖ canonicalize_text:")
    print(f"   Input:  '{text6}'")
    print(f"   Output: '{result6}'\n")
    
    print("üéâ All tests completed!")