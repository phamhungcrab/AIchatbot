from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os
import torch

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, 'models', 'my_generative_bot')

print(f"üìÇ ƒêang t·∫£i model t·ª´: {MODEL_PATH}")

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)
    print("‚úÖ Model loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading model: {e}")
    exit()

def test_generate(question):
    print(f"\n‚ùì C√¢u h·ªèi: {question}")
    input_text = f"question: {question}"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids
    
    # Th·ª≠ c√°c tham s·ªë kh√°c nhau
    print("--- Th·ª≠ nghi·ªám 1 (M·∫∑c ƒë·ªãnh) ---")
    outputs = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)
    print(f"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

    print("--- Th·ª≠ nghi·ªám 2 (Repetition Penalty) ---")
    outputs = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True, repetition_penalty=2.5)
    print(f"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")
    
    print("--- Th·ª≠ nghi·ªám 4 (Aggressive Decoding) ---")
    outputs = model.generate(
        input_ids, 
        max_length=128, 
        num_beams=4, 
        repetition_penalty=3.0, 
        no_repeat_ngram_size=2,
        early_stopping=True
    )
    print(f"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

test_generate("BFS l√† g√¨")
test_generate("DFS l√† g√¨")
