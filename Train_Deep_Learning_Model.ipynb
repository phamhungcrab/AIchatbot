{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† AI Chatbot - Deep Learning Training Notebook\n",
                "\n",
                "Notebook n√†y gi√∫p b·∫°n hu·∫•n luy·ªán m√¥ h√¨nh LSTM/GRU cho Chatbot tr√™n Google Colab.\n",
                "\n",
                "### üìù H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n",
                "1.  **Upload file `train_data_full.csv`** (ƒë∆∞·ª£c t·∫°o t·ª´ script `app/export_data.py`) l√™n Colab.\n",
                "2.  Ch·∫°y l·∫ßn l∆∞·ª£t c√°c √¥ code b√™n d∆∞·ªõi (nh·∫•n n√∫t Play ‚ñ∂Ô∏è).\n",
                "3.  Sau khi ch·∫°y xong, notebook s·∫Ω t·ª± ƒë·ªông n√©n v√† t·∫£i v·ªÅ file `models.zip`.\n",
                "4.  Gi·∫£i n√©n file n√†y v√† ch√©p v√†o th∆∞ m·ª•c `models/` trong d·ª± √°n c·ªßa b·∫°n."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üì¶ 1. C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
                "!pip install pyvi tensorflow pandas scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìö 2. Import th∆∞ vi·ªán\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import os\n",
                "import re\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from pyvi import ViTokenizer \n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üßπ 3. ƒê·ªãnh nghƒ©a Class Preprocessor (Copy t·ª´ app/preprocess.py)\n",
                "# ---------------------------------------------------------\n",
                "\n",
                "class TextPreprocessor:\n",
                "    _instance = None\n",
                "\n",
                "    def __new__(cls):\n",
                "        if cls._instance is None:\n",
                "            cls._instance = super(TextPreprocessor, cls).__new__(cls)\n",
                "            cls._instance._initialize()\n",
                "        return cls._instance\n",
                "\n",
                "    def _initialize(self):\n",
                "        self.re_special_chars = re.compile(r'[^\\w\\s]')\n",
                "        self.re_numbers = re.compile(r'\\d+')\n",
                "        self._load_dictionaries()\n",
                "\n",
                "    def _load_dictionaries(self):\n",
                "        self.VIETNAMESE_STOPWORDS = {\n",
                "            'th√¨', 'l√†', 'm√†', 'v√†', 'c·ªßa', 'nh·ªØng', 'c√°c', 'nh∆∞', 'th·∫ø', 'n√†o', \n",
                "            'ƒë∆∞·ª£c', 'v·ªÅ', 'v·ªõi', 'trong', 'c√≥', 'kh√¥ng', 'cho', 't√¥i', 'b·∫°n', \n",
                "            'c·∫≠u', 't·ªõ', 'm√¨nh', 'n√≥', 'h·∫Øn', 'g√¨', 'c√°i', 'con', 'ng∆∞·ªùi', \n",
                "            's·ª±', 'vi·ªác', 'ƒë√≥', 'ƒë√¢y', 'kia', 'n√†y', 'nh√©', '·∫°', '∆°i', 'ƒëi', \n",
                "            'l√†m', 'khi', 'l√∫c', 'n∆°i', 't·∫°i', 'ƒë√£', 'ƒëang', 's·∫Ω', 'mu·ªën', \n",
                "            'ph·∫£i', 'bi·∫øt', 'h√£y', 'r·ªìi', 'ch·ª©', 'nh·ªâ'\n",
                "        }\n",
                "\n",
                "    def preprocess_text(self, text: str) -> str:\n",
                "        if not text: return \"\"\n",
                "        text = text.lower()\n",
                "        text = self.re_special_chars.sub('', text)\n",
                "        text = self.re_numbers.sub('', text)\n",
                "        tokenized_text = ViTokenizer.tokenize(text)\n",
                "        tokens = tokenized_text.split()\n",
                "        filtered_tokens = [\n",
                "            word for word in tokens \n",
                "            if word not in self.VIETNAMESE_STOPWORDS and len(word) > 1\n",
                "        ]\n",
                "        return ' '.join(filtered_tokens)\n",
                "\n",
                "# Kh·ªüi t·∫°o Preprocessor\n",
                "preprocessor = TextPreprocessor()\n",
                "def preprocess_text(text: str) -> str:\n",
                "    return preprocessor.preprocess_text(text)\n",
                "\n",
                "print(\"‚úÖ Preprocessor initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üèóÔ∏è 4. ƒê·ªãnh nghƒ©a Model Deep Learning (Copy t·ª´ app/dl_model.py)\n",
                "# ---------------------------------------------------------\n",
                "\n",
                "def create_model(vocab_size, embedding_dim, max_length, num_classes):\n",
                "    model = Sequential()\n",
                "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
                "    model.add(SpatialDropout1D(0.2))\n",
                "    model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
                "    model.add(Dense(num_classes, activation='softmax'))\n",
                "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "    return model\n",
                "\n",
                "print(\"‚úÖ Model architecture defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üî• 5. Hu·∫•n luy·ªán M√¥ h√¨nh (Logic t·ª´ app/train_dl.py)\n",
                "# ---------------------------------------------------------\n",
                "\n",
                "# C·∫•u h√¨nh\n",
                "MAX_NUM_WORDS = 5000\n",
                "MAX_SEQUENCE_LENGTH = 100\n",
                "EMBEDDING_DIM = 100\n",
                "EPOCHS = 10\n",
                "BATCH_SIZE = 32\n",
                "MODEL_PATH = 'models/dl_model.h5'\n",
                "TOKENIZER_PATH = 'models/tokenizer.pickle'\n",
                "LABEL_ENCODER_PATH = 'models/label_encoder.pickle'\n",
                "\n",
                "def train_dl_model():\n",
                "    print(\"üöÄ Starting Deep Learning Model Training...\")\n",
                "\n",
                "    # 1. Load Data\n",
                "    if not os.path.exists('train_data_full.csv'):\n",
                "        print(\"‚ùå Error: train_data_full.csv not found. Please upload it!\")\n",
                "        return\n",
                "\n",
                "    df = pd.read_csv('train_data_full.csv')\n",
                "    \n",
                "    # 2. Preprocess Data\n",
                "    print(\"üßπ Preprocessing text...\")\n",
                "    df['clean_text'] = df['question'].apply(preprocess_text)\n",
                "    \n",
                "    # 3. Tokenization & Padding\n",
                "    print(\"üî† Tokenizing...\")\n",
                "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n', lower=True)\n",
                "    tokenizer.fit_on_texts(df['clean_text'].values)\n",
                "    \n",
                "    X = tokenizer.texts_to_sequences(df['clean_text'].values)\n",
                "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
                "\n",
                "    # 4. Encode Labels\n",
                "    print(\"üè∑Ô∏è Encoding labels...\")\n",
                "    if 'topic' not in df.columns:\n",
                "        print(\"‚ùå Error: 'topic' column missing in dataset!\")\n",
                "        return\n",
                "    else:\n",
                "        target_column = 'topic'\n",
                "        \n",
                "    le = LabelEncoder()\n",
                "    Y = le.fit_transform(df[target_column])\n",
                "    num_classes = len(le.classes_)\n",
                "    print(f\"Number of classes: {num_classes}\")\n",
                "\n",
                "    # 5. Build Model\n",
                "    print(\"üèóÔ∏è Building model...\")\n",
                "    model = create_model(MAX_NUM_WORDS, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, num_classes)\n",
                "    model.summary()\n",
                "\n",
                "    # 6. Train Model\n",
                "    print(\"üî• Training...\")\n",
                "    model.fit(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.1, verbose=1)\n",
                "\n",
                "    # 7. Save Artifacts\n",
                "    print(\"üíæ Saving artifacts...\")\n",
                "    if not os.path.exists('models'):\n",
                "        os.makedirs('models')\n",
                "        \n",
                "    model.save(MODEL_PATH)\n",
                "    \n",
                "    with open(TOKENIZER_PATH, 'wb') as handle:\n",
                "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
                "        \n",
                "    with open(LABEL_ENCODER_PATH, 'wb') as handle:\n",
                "        pickle.dump(le, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
                "\n",
                "    print(\"‚úÖ Training complete! Model saved to models/\")\n",
                "\n",
                "# Run Training\n",
                "train_dl_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üì¶ 6. N√©n v√† T·∫£i v·ªÅ Model\n",
                "!zip -r models.zip models/\n",
                "from google.colab import files\n",
                "files.download('models.zip')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}