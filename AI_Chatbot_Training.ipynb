{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ Hu·∫•n luy·ªán Chatbot AI tr√™n Google Colab\n",
                "\n",
                "Notebook n√†y gi√∫p b·∫°n t·∫≠n d·ª•ng GPU mi·ªÖn ph√≠ c·ªßa Google ƒë·ªÉ hu·∫•n luy·ªán model nhanh h∆°n.\n",
                "\n",
                "### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n",
                "1. V√†o menu **Runtime** -> **Change runtime type** -> Ch·ªçn **T4 GPU**.\n",
                "2. Upload file `train_data.csv` c·ªßa b·∫°n l√™n Colab (k√©o th·∫£ v√†o m·ª•c Files b√™n tr√°i).\n",
                "3. Ch·∫°y l·∫ßn l∆∞·ª£t c√°c b∆∞·ªõc b√™n d∆∞·ªõi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
                "!pip install transformers torch sentencepiece accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Import th∆∞ vi·ªán\n",
                "import pandas as pd\n",
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
                "import os\n",
                "\n",
                "# C·∫•u h√¨nh\n",
                "MODEL_NAME = \"google/mt5-small\"\n",
                "MAX_LEN = 128\n",
                "EPOCHS = 5\n",
                "BATCH_SIZE = 8  # TƒÉng l√™n 8 v√¨ Colab GPU m·∫°nh h∆°n m√°y c√° nh√¢n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Chu·∫©n b·ªã d·ªØ li·ªáu (Dataset Class)\n",
                "class QADataset(Dataset):\n",
                "    def __init__(self, tokenizer, data, max_len):\n",
                "        self.tokenizer = tokenizer\n",
                "        self.data = data\n",
                "        self.max_len = max_len\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        row = self.data.iloc[index]\n",
                "        question = str(row['question'])\n",
                "        answer = str(row['answer'])\n",
                "\n",
                "        # Format input cho T5: \"question: ...\"\n",
                "        input_text = f\"question: {question}\"\n",
                "        target_text = answer\n",
                "\n",
                "        # Tokenize Input\n",
                "        input_enc = self.tokenizer(\n",
                "            input_text,\n",
                "            max_length=self.max_len,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors=\"pt\"\n",
                "        )\n",
                "\n",
                "        # Tokenize Output (Label)\n",
                "        target_enc = self.tokenizer(\n",
                "            target_text,\n",
                "            max_length=self.max_len,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors=\"pt\"\n",
                "        )\n",
                "        \n",
                "        # Mask padding tokens in labels\n",
                "        labels = target_enc.input_ids.flatten()\n",
                "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
                "\n",
                "        return {\n",
                "            'input_ids': input_enc.input_ids.flatten(),\n",
                "            'attention_mask': input_enc.attention_mask.flatten(),\n",
                "            'labels': labels\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán\n",
                "def train_model():\n",
                "    # Ki·ªÉm tra GPU\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    print(f\"üî• ƒêang ch·∫°y tr√™n: {device}\")\n",
                "\n",
                "    # Load Tokenizer & Model\n",
                "    print(f\"üì• ƒêang t·∫£i model {MODEL_NAME}...\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
                "    model.to(device)\n",
                "\n",
                "    # Load Data\n",
                "    if not os.path.exists('train_data.csv'):\n",
                "        print(\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file train_data.csv. H√£y upload l√™n Colab tr∆∞·ªõc!\")\n",
                "        return\n",
                "    \n",
                "    print(\"üìö ƒêang ƒë·ªçc d·ªØ li·ªáu...\")\n",
                "    df = pd.read_csv('train_data.csv')\n",
                "    dataset = QADataset(tokenizer, df, MAX_LEN)\n",
                "\n",
                "    # C·∫•u h√¨nh Training\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir='./results',\n",
                "        num_train_epochs=EPOCHS,\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        logging_dir='./logs',\n",
                "        logging_steps=10,\n",
                "        save_strategy=\"no\",  # Kh√¥ng save checkpoint r√°c ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng\n",
                "        report_to=\"none\",\n",
                "        remove_unused_columns=False\n",
                "    )\n",
                "\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=dataset\n",
                "    )\n",
                "\n",
                "    print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...\")\n",
                "    trainer.train()\n",
                "\n",
                "    # L∆∞u model cu·ªëi c√πng\n",
                "    print(\"üíæ ƒêang l∆∞u model...\")\n",
                "    save_path = \"./my_generative_bot\"\n",
                "    model.save_pretrained(save_path)\n",
                "    tokenizer.save_pretrained(save_path)\n",
                "    print(f\"‚úÖ Ho√†n t·∫•t! Model ƒë√£ l∆∞u t·∫°i: {save_path}\")\n",
                "\n",
                "train_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. N√©n v√† T·∫£i model v·ªÅ m√°y\n",
                "!zip -r my_generative_bot.zip my_generative_bot\n",
                "from google.colab import files\n",
                "files.download('my_generative_bot.zip')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}