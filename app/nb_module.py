# -------------------------------
# ğŸ¤– nb_module.py â€” MÃ´-Ä‘un NaÃ¯ve Bayes cho Chatbot
# Chá»©c nÄƒng: dá»± Ä‘oÃ¡n chá»§ Ä‘á» (topic) cá»§a cÃ¢u há»i Ä‘áº§u vÃ o
# vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh NaÃ¯ve Bayes lÆ°u vÃ o file nb_model.pkl
# -------------------------------

import numpy as np
import pickle
import os

# -------------------------------
# ğŸ“ Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n lÆ°u mÃ´ hÃ¬nh
# -------------------------------
BASE_DIR = os.path.dirname(__file__)                   # â†’ thÆ° má»¥c hiá»‡n táº¡i ("app/")
MODEL_DIR = os.path.join(BASE_DIR, '..', 'models')     # â†’ lÃ¹i lÃªn má»™t cáº¥p (AIChatbot/models)
# os.makedirs(MODEL_DIR, exist_ok=True)                # CÃ³ thá»ƒ báº­t láº¡i náº¿u muá»‘n tá»± táº¡o thÆ° má»¥c
MODEL_PATH = os.path.join(MODEL_DIR, 'nb_model.pkl')   # ÄÆ°á»ng dáº«n file model NaÃ¯ve Bayes

# =========================================================
# ğŸ”® 1ï¸âƒ£ HÃ€M Dá»° ÄOÃN CHá»¦ Äá»€ CÃ‚U Há»I (Sá»¬ Dá»¤NG MÃ” HÃŒNH ÄÃƒ HUáº¤N LUYá»†N)
# =========================================================
def predict_topic(nb_model, vectorizer, text):
    """
    âœ… Má»¥c Ä‘Ã­ch:
        Dá»± Ä‘oÃ¡n chá»§ Ä‘á» (topic) cá»§a cÃ¢u há»i ngÆ°á»i dÃ¹ng báº±ng mÃ´ hÃ¬nh NaÃ¯ve Bayes.

    ğŸ“Œ Tham sá»‘:
        nb_model   : mÃ´ hÃ¬nh NaÃ¯ve Bayes Ä‘Ã£ Ä‘Æ°á»£c load sáºµn tá»« file nb_model.pkl
        vectorizer : mÃ´ hÃ¬nh TF-IDF hoáº·c CountVectorizer (Ä‘Ã£ huáº¥n luyá»‡n cÃ¹ng model)
        text       : cÃ¢u há»i ngÆ°á»i dÃ¹ng (chuá»—i string)

    ğŸ” Tráº£ vá»:
        (predicted_topic, confidence)
        - predicted_topic: tÃªn chá»§ Ä‘á» Ä‘Æ°á»£c dá»± Ä‘oÃ¡n (vÃ­ dá»¥: "MachineLearning")
        - confidence: Ä‘á»™ tin cáº­y cá»§a dá»± Ä‘oÃ¡n (xÃ¡c suáº¥t lá»›n nháº¥t)
    """

    # ğŸ§© 1. Biáº¿n Ä‘á»•i vÄƒn báº£n Ä‘áº§u vÃ o thÃ nh vector TF-IDF
    # Vectorizer Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n sáºµn trÃªn dá»¯ liá»‡u Q&A, giÃºp mÃ´ hÃ¬nh hiá»ƒu "ngá»¯ nghÄ©a" cÆ¡ báº£n
    X = vectorizer.transform([text])

    # ğŸ§  2. Dá»± Ä‘oÃ¡n nhÃ£n chá»§ Ä‘á» báº±ng mÃ´ hÃ¬nh NaÃ¯ve Bayes
    predicted_topic = nb_model.predict(X)[0]

    # ğŸ“ˆ 3. TÃ­nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho táº¥t cáº£ cÃ¡c chá»§ Ä‘á»
    probs = nb_model.predict_proba(X)
    confidence = np.max(probs)  # Láº¥y giÃ¡ trá»‹ xÃ¡c suáº¥t cao nháº¥t lÃ m Ä‘á»™ tin cáº­y

    # ğŸ’¬ 4. Tráº£ vá» káº¿t quáº£ (chá»§ Ä‘á», Ä‘á»™ tin cáº­y)
    return predicted_topic, round(float(confidence), 4)


# =========================================================
# ğŸ§  2ï¸âƒ£ HÃ€M HUáº¤N LUYá»†N MÃ” HÃŒNH NAÃVE BAYES (cháº¡y offline)
# =========================================================
class CustomMultinomialNB:
    """
    Tá»± cÃ i Ä‘áº·t thuáº­t toÃ¡n Multinomial Naive Bayes (tÆ°Æ¡ng tá»± sklearn).
    """
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        self.class_log_prior_ = None
        self.feature_log_prob_ = None
        self.classes_ = None

    def fit(self, X, y):
        """
        Huáº¥n luyá»‡n mÃ´ hÃ¬nh.
        X: Ma tráº­n Ä‘áº·c trÆ°ng (sparse matrix hoáº·c array), shape (n_samples, n_features)
        y: NhÃ£n (array), shape (n_samples,)
        """
        # Chuyá»ƒn y thÃ nh array náº¿u chÆ°a pháº£i
        y = np.array(y)
        
        # XÃ¡c Ä‘á»‹nh cÃ¡c lá»›p (classes)
        self.classes_ = np.unique(y)
        n_classes = len(self.classes_)
        n_features = X.shape[1]

        # Khá»Ÿi táº¡o cÃ¡c biáº¿n Ä‘áº¿m
        self.class_log_prior_ = np.zeros(n_classes)
        self.feature_log_prob_ = np.zeros((n_classes, n_features))

        # TÃ­nh toÃ¡n cho tá»«ng lá»›p
        for idx, c in enumerate(self.classes_):
            # Láº¥y cÃ¡c máº«u thuá»™c lá»›p c
            X_c = X[y == c]
            
            # TÃ­nh xÃ¡c suáº¥t tiÃªn nghiá»‡m (Prior) P(c)
            # Log probability = log(sá»‘ máº«u lá»›p c / tá»•ng sá»‘ máº«u)
            self.class_log_prior_[idx] = np.log(X_c.shape[0] / X.shape[0])

            # TÃ­nh tá»•ng sá»‘ láº§n xuáº¥t hiá»‡n cá»§a tá»«ng tá»« trong lá»›p c
            # Cá»™ng thÃªm alpha Ä‘á»ƒ lÃ m smoothing (trÃ¡nh xÃ¡c suáº¥t = 0)
            count_word_in_class = X_c.sum(axis=0) + self.alpha
            
            # Tá»•ng sá»‘ tá»« trong toÃ n bá»™ lá»›p c (bao gá»“m cáº£ alpha cho má»—i tá»«)
            total_count_in_class = count_word_in_class.sum()

            # TÃ­nh xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n (Likelihood) P(x_i | c)
            # Log probability = log(sá»‘ láº§n tá»« i xuáº¥t hiá»‡n trong c / tá»•ng sá»‘ tá»« trong c)
            self.feature_log_prob_[idx, :] = np.log(count_word_in_class / total_count_in_class)
            
        return self

    def predict_log_proba(self, X):
        """
        TÃ­nh log xÃ¡c suáº¥t háº­u nghiá»‡m: log P(c | X) ~ log P(c) + sum(log P(x_i | c))
        """
        # X * feature_log_prob_.T:
        # (n_samples, n_features) x (n_features, n_classes) -> (n_samples, n_classes)
        # ÄÃ¢y lÃ  bÆ°á»›c nhÃ¢n ma tráº­n Ä‘á»ƒ cá»™ng tá»•ng log likelihood cá»§a cÃ¡c tá»« trong cÃ¢u
        jll = X @ self.feature_log_prob_.T + self.class_log_prior_
        return jll

    def predict_proba(self, X):
        """
        Chuyá»ƒn Ä‘á»•i log proba sang xÃ¡c suáº¥t thá»±c (dÃ¹ng hÃ m softmax hoáº·c chuáº©n hÃ³a).
        """
        jll = self.predict_log_proba(X)
        # Ká»¹ thuáº­t log-sum-exp Ä‘á»ƒ trÃ¡nh trÃ n sá»‘ (overflow/underflow)
        # P(c|X) = exp(log P(c|X)) / sum(exp(log P(c'|X)))
        
        # Trá»« max Ä‘á»ƒ á»•n Ä‘á»‹nh sá»‘ há»c
        jll_stable = jll - jll.max(axis=1, keepdims=True)
        exp_jll = np.exp(jll_stable)
        prob = exp_jll / exp_jll.sum(axis=1, keepdims=True)
        return prob

    def predict(self, X):
        """
        Dá»± Ä‘oÃ¡n lá»›p cÃ³ xÃ¡c suáº¥t cao nháº¥t.
        """
        jll = self.predict_log_proba(X)
        return self.classes_[np.argmax(jll, axis=1)]
    
    def score(self, X, y):
        """
        TÃ­nh Ä‘á»™ chÃ­nh xÃ¡c (Accuracy).
        """
        y_pred = self.predict(X)
        return np.mean(y_pred == y)


# =========================================================
# ğŸ”„ 4ï¸âƒ£ HÃ€M CROSS-VALIDATION Tá»° VIáº¾T
# =========================================================
def custom_cross_val_score(model, X, y, cv=5):
    """
    Tá»± cÃ i Ä‘áº·t K-Fold Cross Validation.
    """
    y = np.array(y)
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    
    # XÃ¡o trá»™n dá»¯ liá»‡u (Ä‘á»ƒ Ä‘áº£m báº£o ngáº«u nhiÃªn)
    np.random.seed(42)
    np.random.shuffle(indices)
    
    fold_sizes = np.full(cv, n_samples // cv, dtype=int)
    fold_sizes[:n_samples % cv] += 1
    
    current = 0
    scores = []
    
    print(f"   ğŸ”„ Running Custom {cv}-Fold CV...")
    
    for i in range(cv):
        start, stop = current, current + fold_sizes[i]
        test_indices = indices[start:stop]
        train_indices = np.concatenate([indices[:start], indices[stop:]])
        
        current = stop
        
        # Chia táº­p train/test
        X_train, X_test = X[train_indices], X[test_indices]
        y_train, y_test = y[train_indices], y[test_indices]
        
        # Clone model (táº¡o má»›i Ä‘á»ƒ khÃ´ng áº£nh hÆ°á»Ÿng model gá»‘c)
        # á» Ä‘Ã¢y ta khá»Ÿi táº¡o má»›i Ä‘Æ¡n giáº£n
        clf = CustomMultinomialNB(alpha=model.alpha)
        clf.fit(X_train, y_train)
        
        # ÄÃ¡nh giÃ¡
        score = clf.score(X_test, y_test)
        scores.append(score)
        
    return np.array(scores)


# =========================================================
# ğŸ§  2ï¸âƒ£ HÃ€M HUáº¤N LUYá»†N MÃ” HÃŒNH NAÃVE BAYES (cháº¡y offline)
# =========================================================
def train_naive_bayes(vectorizer, train_texts, train_labels):
    """
    âœ… Má»¥c Ä‘Ã­ch:
        Huáº¥n luyá»‡n mÃ´ hÃ¬nh NaÃ¯ve Bayes Ä‘á»ƒ phÃ¢n loáº¡i cÃ¢u há»i vÃ o cÃ¡c chá»§ Ä‘á» (topics)
        vÃ  lÆ°u láº¡i model Ä‘Ã£ huáº¥n luyá»‡n vÃ o file nb_model.pkl.

    ğŸ“Œ Tham sá»‘:
        vectorizer   : mÃ´ hÃ¬nh TF-IDF hoáº·c CountVectorizer (Ä‘Ã£ fit sáºµn)
        train_texts  : danh sÃ¡ch cÃ¡c cÃ¢u há»i huáº¥n luyá»‡n (list[str])
        train_labels : danh sÃ¡ch nhÃ£n chá»§ Ä‘á» tÆ°Æ¡ng á»©ng (list[str])

    ğŸ” Tráº£ vá»:
        nb_model: mÃ´ hÃ¬nh NaÃ¯ve Bayes Ä‘Ã£ huáº¥n luyá»‡n
    """
    # (1) TÃ¹y chá»‰nh vectorizer Ä‘á»ƒ tá»‘i Æ°u cho Naive Bayes
    vectorizer.set_params(
        max_features=800,        # Giá»›i háº¡n sá»‘ Ä‘áº·c trÆ°ng Ä‘á»ƒ trÃ¡nh ma tráº­n quÃ¡ thÆ°a
        ngram_range=(1, 2),      # Há»c cáº£ tá»« Ä‘Æ¡n (unigram) vÃ  cá»¥m 2 tá»« (bigram)
        min_df=1                 # Giá»¯ tá»« xuáº¥t hiá»‡n Ã­t nháº¥t 1 láº§n
    )

    # Biáº¿n Ä‘á»•i dá»¯ liá»‡u huáº¥n luyá»‡n thÃ nh vector TF-IDF vá»›i cáº¥u hÃ¬nh má»›i
    X_train = vectorizer.fit_transform(train_texts)

    # (2) Huáº¥n luyá»‡n mÃ´ hÃ¬nh Custom Multinomial NaÃ¯ve Bayes
    # alpha=0.1: Giáº£m smoothing Ä‘á»ƒ model "nháº¡y" hÆ¡n vá»›i cÃ¡c tá»« khÃ³a Ä‘áº·c trÆ°ng
    nb_model = CustomMultinomialNB(alpha=0.1)
    nb_model.fit(X_train, train_labels)

    # (3) ÄÃ¡nh giÃ¡ sÆ¡ bá»™ báº±ng custom cross-validation
    scores = custom_cross_val_score(nb_model, X_train, train_labels, cv=5)
    print(f"ğŸ“Š Custom CV accuracy: {np.mean(scores):.3f} Â± {np.std(scores):.3f}")

    # ğŸ’¾ LÆ°u mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n láº¡i Ä‘á»ƒ dÃ¹ng sau
    with open(MODEL_PATH, 'wb') as f:
        pickle.dump(nb_model, f)

    # ThÃ´ng bÃ¡o khi lÆ°u thÃ nh cÃ´ng
    print("âœ… Optimized Custom NaÃ¯ve Bayes model saved at:", os.path.abspath(MODEL_PATH))

    return nb_model
