# -------------------------------
# ğŸ§¹ preprocess.py â€” MÃ´-Ä‘un tiá»n xá»­ lÃ½ vÄƒn báº£n
# Chá»©c nÄƒng: lÃ m sáº¡ch, chuáº©n hÃ³a dá»¯ liá»‡u ngÃ´n ngá»¯ tá»± nhiÃªn
# Ä‘á»ƒ mÃ´ hÃ¬nh NaÃ¯ve Bayes & KNN hiá»ƒu Ä‘Æ°á»£c.
# -------------------------------

import re                # Regular Expressions â†’ xá»­ lÃ½ kÃ½ tá»± Ä‘áº·c biá»‡t, lá»c chuá»—i
import string            # DÃ¹ng Ä‘á»ƒ truy cáº­p dáº¥u cÃ¢u (punctuation)
import nltk              # Natural Language Toolkit â€” thÆ° viá»‡n xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn
# DÃ¹ng Ä‘á»ƒ vector hÃ³a vÄƒn báº£n (TF-IDF)
from sklearn.feature_extraction.text import TfidfVectorizer

# -------------------------------
# âš™ï¸ Thiáº¿t láº­p stopwords (tá»« dá»«ng)
# -------------------------------
# Náº¿u lÃ  láº§n Ä‘áº§u cháº¡y trÃªn mÃ¡y má»›i, báº¡n cáº§n táº£i vá» dá»¯ liá»‡u stopwords:
# â†’ Bá» dáº¥u "#" á»Ÿ 2 dÃ²ng sau vÃ  cháº¡y má»™t láº§n
# nltk.download('stopwords')
# nltk.download('punkt')

from nltk.corpus import stopwords
# Táº­p há»£p cÃ¡c tá»« dá»«ng tiáº¿ng Anh (cÃ³ thá»ƒ má»Ÿ rá»™ng thÃªm tiáº¿ng Viá»‡t)
stop_words = set(stopwords.words('english'))


# =========================================================
# ğŸ§  1ï¸âƒ£ HÃ€M TIá»€N Xá»¬ LÃ CHUá»–I VÄ‚N Báº¢N
# =========================================================
def preprocess_text(text: str) -> str:
    """
    âœ… Má»¥c Ä‘Ã­ch:
        LÃ m sáº¡ch vÃ  chuáº©n hÃ³a vÄƒn báº£n Ä‘áº§u vÃ o Ä‘á»ƒ mÃ´ hÃ¬nh há»c mÃ¡y xá»­ lÃ½ tá»‘t hÆ¡n.

    ğŸ“Œ CÃ¡c bÆ°á»›c thá»±c hiá»‡n:
        1ï¸âƒ£ Chuyá»ƒn toÃ n bá»™ sang chá»¯ thÆ°á»ng.
        2ï¸âƒ£ Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, dáº¥u cÃ¢u, vÃ  sá»‘.
        3ï¸âƒ£ TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c tá»« riÃªng láº» (tokenize).
        4ï¸âƒ£ XÃ³a bá» cÃ¡c tá»« dá»«ng (stopwords) khÃ´ng mang nhiá»u Ã½ nghÄ©a.
        5ï¸âƒ£ GhÃ©p láº¡i thÃ nh chuá»—i sáº¡ch cuá»‘i cÃ¹ng.

    ğŸ” Tráº£ vá»:
        clean_text: chuá»—i vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½.
    """

    # 1ï¸âƒ£ Chuyá»ƒn táº¥t cáº£ kÃ½ tá»± vá» chá»¯ thÆ°á»ng Ä‘á»ƒ thá»‘ng nháº¥t
    text = text.lower()

    # 2ï¸âƒ£ Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, dáº¥u cháº¥m, dáº¥u há»i, v.v.
    # [^\w\s] nghÄ©a lÃ  giá»¯ láº¡i kÃ½ tá»± chá»¯ vÃ  khoáº£ng tráº¯ng, bá» táº¥t cáº£ cÃ²n láº¡i
    text = re.sub(r'[^\w\s]', '', text)

    # 3ï¸âƒ£ Loáº¡i bá» chá»¯ sá»‘ (sá»‘ 0â€“9) Ä‘á»ƒ trÃ¡nh nhiá»…u
    text = re.sub(r'\d+', '', text)

    # 4ï¸âƒ£ Tokenization â€” tÃ¡ch vÄƒn báº£n thÃ nh danh sÃ¡ch cÃ¡c tá»« (tokens)
    tokens = nltk.word_tokenize(text)

    # 5ï¸âƒ£ Loáº¡i bá» stopwords (vÃ­ dá»¥: "the", "is", "are", "and"...)
    # â†’ giÃºp mÃ´ hÃ¬nh táº­p trung vÃ o tá»« khÃ³a chÃ­nh
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # 6ï¸âƒ£ GhÃ©p láº¡i danh sÃ¡ch tá»« thÃ nh chuá»—i hoÃ n chá»‰nh (ngÄƒn cÃ¡ch báº±ng khoáº£ng tráº¯ng)
    clean_text = ' '.join(filtered_tokens)

    # ğŸ Tráº£ vá» vÄƒn báº£n Ä‘Ã£ lÃ m sáº¡ch
    return clean_text


# =========================================================
# ğŸ“Š 2ï¸âƒ£ HÃ€M HUáº¤N LUYá»†N TF-IDF VECTORIZER
# =========================================================
def train_vectorizer(corpus):
    """
    âœ… Má»¥c Ä‘Ã­ch:
        Huáº¥n luyá»‡n TF-IDF vectorizer tá»« danh sÃ¡ch vÄƒn báº£n (corpus).
        Sau Ä‘Ã³ cÃ³ thá»ƒ lÆ°u vectorizer vÃ o file .pkl Ä‘á»ƒ sá»­ dá»¥ng láº¡i.

    ğŸ“Œ Tham sá»‘:
        corpus: danh sÃ¡ch chuá»—i vÄƒn báº£n (list[str]), vÃ­ dá»¥ lÃ  cÃ¡c cÃ¢u há»i trong cÆ¡ sá»Ÿ dá»¯ liá»‡u

    ğŸ” Tráº£ vá»:
        vectorizer: Ä‘á»‘i tÆ°á»£ng TF-IDF Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n
    """

    # max_features=3000 â†’ chá»‰ giá»¯ láº¡i 3000 tá»« quan trá»ng nháº¥t (giÃºp giáº£m kÃ­ch thÆ°á»›c)
    vectorizer = TfidfVectorizer(max_features=3000)

    # "fit" Ä‘á»ƒ há»c ra bá»™ tá»« vá»±ng vÃ  trá»ng sá»‘ TF-IDF
    vectorizer.fit(corpus)

    # ğŸ Tráº£ vá» mÃ´ hÃ¬nh vectorizer Ä‘Ã£ huáº¥n luyá»‡n
    return vectorizer
