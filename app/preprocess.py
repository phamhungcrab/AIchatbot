# -------------------------------
# ğŸ§¹ preprocess.py â€” Tiá»n xá»­ lÃ½ vÄƒn báº£n Tiáº¿ng Viá»‡t tá»‘i Æ°u
# -------------------------------

import re
import pickle
# ThÆ° viá»‡n tÃ¡ch tá»« chuyÃªn dá»¥ng cho tiáº¿ng Viá»‡t
from pyvi import ViTokenizer 
# ThÆ° viá»‡n vector hÃ³a vÄƒn báº£n
from sklearn.feature_extraction.text import TfidfVectorizer

# -------------------------------
# ğŸ›‘ 1. DANH SÃCH STOPWORDS TIáº¾NG VIá»†T (Tá»ª Dá»ªNG)
# -------------------------------
# ÄÃ¢y lÃ  nhá»¯ng tá»« xuáº¥t hiá»‡n nhiá»u nhÆ°ng Ã­t mang Ã½ nghÄ©a phÃ¢n loáº¡i.
# Loáº¡i bá» chÃºng giÃºp bot táº­p trung vÃ o tá»« khÃ³a chÃ­nh (nhÆ° "há»c mÃ¡y", "giáº£i thuáº­t").
VIETNAMESE_STOPWORDS = {
    'thÃ¬', 'lÃ ', 'mÃ ', 'vÃ ', 'cá»§a', 'nhá»¯ng', 'cÃ¡c', 'nhÆ°', 'tháº¿', 'nÃ o', 
    'Ä‘Æ°á»£c', 'vá»', 'vá»›i', 'trong', 'cÃ³', 'khÃ´ng', 'cho', 'tÃ´i', 'báº¡n', 
    'cáº­u', 'tá»›', 'mÃ¬nh', 'nÃ³', 'háº¯n', 'gÃ¬', 'cÃ¡i', 'con', 'ngÆ°á»i', 
    'sá»±', 'viá»‡c', 'Ä‘Ã³', 'Ä‘Ã¢y', 'kia', 'nÃ y', 'nhÃ©', 'áº¡', 'Æ¡i', 'Ä‘i', 
    'lÃ m', 'khi', 'lÃºc', 'nÆ¡i', 'táº¡i', 'Ä‘Ã£', 'Ä‘ang', 'sáº½', 'muá»‘n', 
    'pháº£i', 'biáº¿t', 'hÃ£y', 'rá»“i', 'chá»©', 'nhá»‰'
}

# =========================================================
# ğŸ§  2. HÃ€M TIá»€N Xá»¬ LÃ CHUá»–I VÄ‚N Báº¢N
# =========================================================
def preprocess_text(text: str) -> str:
    """
    Quy trÃ¬nh: Lowercase -> XÃ³a kÃ½ tá»± láº¡ -> TÃ¡ch tá»« (PyVi) -> Lá»c Stopwords
    """
    if not text:
        return ""

    # 1ï¸âƒ£ Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng
    text = text.lower()

    # 2ï¸âƒ£ XÃ³a cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t (giá»¯ láº¡i chá»¯ cÃ¡i, sá»‘ vÃ  dáº¥u cÃ¡ch)
    # Loáº¡i bá» dáº¥u cháº¥m, pháº©y, há»i cháº¥m... Ä‘á»ƒ trÃ¡nh nhiá»…u
    text = re.sub(r'[^\w\s]', '', text)
    
    # 3ï¸âƒ£ Loáº¡i bá» sá»‘ (TÃ¹y chá»n: Náº¿u bot cáº§n xá»­ lÃ½ toÃ¡n há»c thÃ¬ bá» dÃ²ng nÃ y)
    text = re.sub(r'\d+', '', text)

    # 4ï¸âƒ£ TÃ¡ch tá»« chuáº©n tiáº¿ng Viá»‡t báº±ng PyVi
    # Quan trá»ng: "há»c mÃ¡y" -> "há»c_mÃ¡y", "trÃ­ tuá»‡ nhÃ¢n táº¡o" -> "trÃ­_tuá»‡_nhÃ¢n_táº¡o"
    # GiÃºp Bot hiá»ƒu Ä‘Ã¢y lÃ  1 cá»¥m tá»« chá»© khÃ´ng pháº£i cÃ¡c tá»« rá»i ráº¡c.
    tokenized_text = ViTokenizer.tokenize(text)

    # 5ï¸âƒ£ TÃ¡ch thÃ nh danh sÃ¡ch Ä‘á»ƒ lá»c Stopwords
    tokens = tokenized_text.split()
    
    # 6ï¸âƒ£ Lá»c bá» tá»« dá»«ng vÃ  cÃ¡c tá»« quÃ¡ ngáº¯n (<= 1 kÃ½ tá»±)
    filtered_tokens = [
        word for word in tokens 
        if word not in VIETNAMESE_STOPWORDS and len(word) > 1
    ]

    # 7ï¸âƒ£ GhÃ©p láº¡i thÃ nh chuá»—i hoÃ n chá»‰nh
    return ' '.join(filtered_tokens)


# =========================================================
# ğŸ“Š 3. HÃ€M HUáº¤N LUYá»†N TF-IDF VECTORIZER (CÃ“ N-GRAM)
# =========================================================
def train_vectorizer(corpus):
    """
    Huáº¥n luyá»‡n bá»™ chuyá»ƒn Ä‘á»•i vÄƒn báº£n sang sá»‘ (Vector).
    Cáº­p nháº­t: Sá»­ dá»¥ng N-gram Ä‘á»ƒ tÄƒng Ä‘á»™ tin cáº­y cho NaÃ¯ve Bayes.
    """
    
    vectorizer = TfidfVectorizer(
        # â­ï¸ Giáº£m xuá»‘ng 800 Ä‘á»ƒ tá»‘i Æ°u cho Naive Bayes (trÃ¡nh ma tráº­n quÃ¡ thÆ°a)
        max_features=800,
        
        # â­ï¸ QUAN TRá»ŒNG: N-gram range (1, 2)
        # GiÃºp model há»c cáº£ tá»« Ä‘Æ¡n ("há»c") vÃ  cá»¥m 2 tá»« ("há»c_mÃ¡y").
        # Äiá»u nÃ y giÃºp tÄƒng Ä‘á»™ tin cáº­y (confidence score) lÃªn ráº¥t nhiá»u.
        ngram_range=(1, 2),
        
        # Bá» qua cÃ¡c tá»« xuáº¥t hiá»‡n quÃ¡ Ã­t (dÆ°á»›i 1 láº§n - máº·c Ä‘á»‹nh)
        min_df=1,
        
        # â­ï¸ Sublinear TF scaling: sá»­ dá»¥ng log(tf) thay vÃ¬ tf
        # GiÃºp giáº£m áº£nh hÆ°á»Ÿng cá»§a tá»« xuáº¥t hiá»‡n quÃ¡ nhiá»u láº§n
        sublinear_tf=True
    )

    # Há»c tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o
    vectorizer.fit(corpus)

    return vectorizer