# -------------------------------
# ðŸ§¹ preprocess.py â€” Tiá»n xá»­ lÃ½ vÄƒn báº£n Tiáº¿ng Viá»‡t tá»‘i Æ°u (Refactored)
# -------------------------------

import re
import pickle
from pyvi import ViTokenizer 
from sklearn.feature_extraction.text import TfidfVectorizer

# =========================================================
# ðŸ§  CLASS TEXT PREPROCESSOR (SINGLETON)
# =========================================================
class TextPreprocessor:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(TextPreprocessor, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        """Khá»Ÿi táº¡o cÃ¡c tÃ i nguyÃªn, compile regex má»™t láº§n duy nháº¥t."""
        
        # 1. Compile Regex Patterns (Tá»‘i Æ°u tá»‘c Ä‘á»™)
        self.re_special_chars = re.compile(r'[^\w\s]')
        self.re_numbers = re.compile(r'\d+')
        
        # 2. Load Data Dictionaries
        self._load_dictionaries()
        
        # 3. Build Synonym Regex (Tá»‘i Æ°u tÃ¬m kiáº¿m O(1))
        # Táº¡o pattern dáº¡ng: \b(phrase1|phrase2|...)\b
        # Sáº¯p xáº¿p theo Ä‘á»™ dÃ i giáº£m dáº§n Ä‘á»ƒ Æ°u tiÃªn cá»¥m tá»« dÃ i trÆ°á»›c (Longest Match)
        all_phrases = [p for p in self.SYNONYMS.keys() if " " in p]
        all_phrases.sort(key=len, reverse=True)
        if all_phrases:
            pattern = r'\b(' + '|'.join(map(re.escape, all_phrases)) + r')\b'
            self.re_synonym_phrases = re.compile(pattern, re.IGNORECASE)
        else:
            self.re_synonym_phrases = None

    def _load_dictionaries(self):
        """Äá»‹nh nghÄ©a cÃ¡c tá»« Ä‘iá»ƒn dá»¯ liá»‡u."""
        self.VIETNAMESE_STOPWORDS = {
            'thÃ¬', 'lÃ ', 'mÃ ', 'vÃ ', 'cá»§a', 'nhá»¯ng', 'cÃ¡c', 'nhÆ°', 'tháº¿', 'nÃ o', 
            'Ä‘Æ°á»£c', 'vá»', 'vá»›i', 'trong', 'cÃ³', 'khÃ´ng', 'cho', 'tÃ´i', 'báº¡n', 
            'cáº­u', 'tá»›', 'mÃ¬nh', 'nÃ³', 'háº¯n', 'gÃ¬', 'cÃ¡i', 'con', 'ngÆ°á»i', 
            'sá»±', 'viá»‡c', 'Ä‘Ã³', 'Ä‘Ã¢y', 'kia', 'nÃ y', 'nhÃ©', 'áº¡', 'Æ¡i', 'Ä‘i', 
            'lÃ m', 'khi', 'lÃºc', 'nÆ¡i', 'táº¡i', 'Ä‘Ã£', 'Ä‘ang', 'sáº½', 'muá»‘n', 
            'pháº£i', 'biáº¿t', 'hÃ£y', 'rá»“i', 'chá»©', 'nhá»‰'
        }
        
        # ðŸ†• Tá»« khÃ³a quan trá»ng KHÃ”NG Ä‘Æ°á»£c xÃ³a khi preprocessing cho KNN
        # Sá»­ dá»¥ng cho semantic matching - cáº§n giá»¯ context
        self.CRITICAL_KEYWORDS = {
            # Thuáº­t toÃ¡n AI/ML
            'knn', 'bfs', 'dfs', 'svm', 'cnn', 'rnn', 'lstm', 'transformer',
            'naive', 'bayes', 'decision', 'tree', 'random', 'forest',
            'gradient', 'descent', 'backpropagation', 'softmax', 'sigmoid',
            # Search algorithms
            'minimax', 'alpha', 'beta', 'heuristic', 'admissible', 'consistent',
            'ucs', 'ids', 'a*', 'greedy',
            # Logic
            'modus', 'ponens', 'resolution', 'cnf', 'fol', 'kb',
            # Tá»« khÃ³a há»i Ä‘Ã¡p quan trá»ng (giá»¯ cho KNN)
            'lÃ ', 'gÃ¬', 'khÃ¡c', 'giá»‘ng', 'so', 'sÃ¡nh', 'táº¡i', 'sao', 'nhÆ°', 'nÃ o',
            # Topics
            'agent', 'tÃ¡c', 'tá»­', 'mÃ´i', 'trÆ°á»ng', 'há»c', 'mÃ¡y', 'sÃ¢u'
        }
        
        # Stopwords nháº¹ cho KNN - chá»‰ xÃ³a cÃ¡c tá»« thá»±c sá»± lÃ  noise
        self.LIGHT_STOPWORDS = {
            'thÃ¬', 'mÃ ', 'vÃ ', 'cá»§a', 'nhá»¯ng', 'cÃ¡c', 'Ä‘Æ°á»£c', 'cho', 'tÃ´i', 'báº¡n',
            'cáº­u', 'tá»›', 'mÃ¬nh', 'nÃ³', 'háº¯n', 'cÃ¡i', 'con', 'sá»±', 'viá»‡c',
            'Ä‘Ã³', 'Ä‘Ã¢y', 'kia', 'nÃ y', 'nhÃ©', 'áº¡', 'Æ¡i', 'Ä‘i', 'rá»“i', 'chá»©', 'nhá»‰'
        }

        self.SYNONYMS = {
            # 1. Thuáº­t toÃ¡n & KhÃ¡i niá»‡m cÆ¡ báº£n
            "knn": ["k-nearest neighbors", "k nearest neighbors", "lÃ¢n cáº­n gáº§n nháº¥t", "k lÃ¢n cáº­n"],
            "naive bayes": ["naÃ¯ve bayes", "bayes ngÃ¢y thÆ¡", "bayes"],
            "bfs": ["breadth-first search", "tÃ¬m kiáº¿m theo chiá»u rá»™ng", "chiá»u rá»™ng"],
            "dfs": ["depth-first search", "tÃ¬m kiáº¿m theo chiá»u sÃ¢u", "chiá»u sÃ¢u"],
            "a*": ["a star", "a sao", "thuáº­t toÃ¡n a*"],
            
            # 2. Logic & Suy diá»…n
            "logic má»‡nh Ä‘á»": ["propositional logic", "logic phÃ¡t biá»ƒu"],
            "logic vá»‹ tá»«": ["first-order logic", "fol", "logic báº­c nháº¥t"],
            "kb": ["knowledge base", "cÆ¡ sá»Ÿ tri thá»©c"],
            
            # 3. Há»c mÃ¡y (Machine Learning)
            "há»c cÃ³ giÃ¡m sÃ¡t": ["supervised learning", "há»c giÃ¡m sÃ¡t"],
            "há»c khÃ´ng giÃ¡m sÃ¡t": ["unsupervised learning", "há»c khÃ´ng giÃ¡m sÃ¡t"],
            "há»c tÄƒng cÆ°á»ng": ["reinforcement learning", "rl"],
            "há»c mÃ¡y": ["machine learning", "ml"],
            "trÃ­ tuá»‡ nhÃ¢n táº¡o": ["ai", "artificial intelligence"],
            "xá»­ lÃ½ ngÃ´n ngá»¯": ["nlp", "natural language processing"],
            
            # 4. TÃ¡c tá»­ & MÃ´i trÆ°á»ng
            "tÃ¡c tá»­": ["agent", "Ä‘áº¡i lÃ½"],
            "peas": ["performance environment actuators sensors", "Ä‘á»™ Ä‘o mÃ´i trÆ°á»ng bá»™ cháº¥p hÃ nh cáº£m biáº¿n"],
            "mÃ´i trÆ°á»ng": ["environment"],
            "cáº£m biáº¿n": ["sensors"],
            "bá»™ cháº¥p hÃ nh": ["actuators"],

            # 6. Tá»« khÃ³a há»i Ä‘Ã¡p thÃ´ng dá»¥ng
            "lÃ  gÃ¬": ["lÃ  cÃ¡i gÃ¬", "nghÄ©a lÃ  gÃ¬", "Ä‘á»‹nh nghÄ©a", "khÃ¡i niá»‡m", "chá»©c nÄƒng", "tÃ¡c dá»¥ng", "cÃ´ng dá»¥ng", "vai trÃ²", "Ã½ nghÄ©a", "dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬"],
            "táº¡i sao": ["vÃ¬ sao", "lÃ½ do", "nguyÃªn nhÃ¢n"],
            "nhÆ° tháº¿ nÃ o": ["ra sao", "lÃ m sao", "cÃ¡ch nÃ o"],
            
            # 5. Thiáº¿t bá»‹ & Äá»i sá»‘ng
            "xe hÆ¡i": ["Ã´ tÃ´", "xáº¿ há»™p", "bá»‘n bÃ¡nh"],
            "Ä‘iá»‡n thoáº¡i": ["dáº¿", "smartphone", "mobile", "di Ä‘á»™ng"],
            "mÃ¡y tÃ­nh": ["laptop", "pc", "computer", "desktop"],
            "kÃ©m": ["tá»‡", "xáº¥u", "yáº¿u", "dá»Ÿ"],
            "tá»‘t": ["ngon", "xá»‹n", "Ä‘á»‰nh", "tuyá»‡t", "hay"]
        }

        self.WEIGHTED_KEYWORDS = {
            "giÃ¡": 2.0, "mua": 1.5, "bÃ¡n": 1.5, "lá»—i": 2.0,
            "khÃ´ng": 1.5, "táº¡i sao": 1.5, "lÃ  gÃ¬": 1.2,
            # ðŸ”¥ Tá»« khÃ³a so sÃ¡nh - Æ°u tiÃªn cao Ä‘á»ƒ nháº­n diá»‡n cÃ¢u há»i so sÃ¡nh
            "khÃ¡c": 3.0, "khÃ¡c gÃ¬": 3.0, "khÃ¡c nhau": 3.0, 
            "so sÃ¡nh": 3.0, "so vá»›i": 2.5, "giá»‘ng": 2.5,
            "khÃ¡c biá»‡t": 3.0, "Ä‘iá»ƒm khÃ¡c": 3.0
        }

        self.NEGATION_WORDS = {"khÃ´ng", "cháº³ng", "cháº£", "Ä‘á»«ng", "chÆ°a", "kÃ©m", "Ä‘Ã¢u"}

        # Táº¡o mapping ngÆ°á»£c (Canonicalization)
        self.REVERSE_SYNONYMS = {}
        for canonical, variations in self.SYNONYMS.items():
            for var in variations:
                self.REVERSE_SYNONYMS[var] = canonical
            self.REVERSE_SYNONYMS[canonical] = canonical

    def preprocess_text(self, text: str) -> str:
        """Quy trÃ¬nh: Lowercase -> XÃ³a kÃ½ tá»± láº¡ -> TÃ¡ch tá»« (PyVi) -> Lá»c Stopwords"""
        if not text: return ""

        # 1. Lowercase & Clean (DÃ¹ng Compiled Regex)
        text = text.lower()
        text = self.re_special_chars.sub('', text)
        text = self.re_numbers.sub('', text)

        # 2. Tokenize (PyVi)
        tokenized_text = ViTokenizer.tokenize(text)

        # 3. Filter Stopwords
        tokens = tokenized_text.split()
        filtered_tokens = [
            word for word in tokens 
            if word not in self.VIETNAMESE_STOPWORDS and len(word) > 1
        ]

        return ' '.join(filtered_tokens)

    def preprocess_for_knn(self, text: str) -> str:
        """
        ðŸ†• Preprocessing nháº¹ cho KNN - giá»¯ láº¡i tá»« khÃ³a quan trá»ng.
        
        KhÃ¡c vá»›i preprocess_text (NB):
        - DÃ¹ng LIGHT_STOPWORDS thay vÃ¬ VIETNAMESE_STOPWORDS 
        - Giá»¯ láº¡i CRITICAL_KEYWORDS (thuáº­t ngá»¯ AI/ML)
        - Má»Ÿ rá»™ng vá»›i synonyms Ä‘á»ƒ tÄƒng matching
        
        Args:
            text: CÃ¢u há»i gá»‘c cá»§a user
            
        Returns:
            str: CÃ¢u Ä‘Ã£ preprocess, phÃ¹ há»£p cho cosine similarity
        """
        if not text: return ""

        # 1. Lowercase & Clean (giá»¯ nguyÃªn nhÆ° preprocess_text)
        text = text.lower()
        text = self.re_special_chars.sub('', text)
        # KHÃ”NG xÃ³a sá»‘ cho KNN (cÃ³ thá»ƒ quan trá»ng: k=5, top-5, etc.)
        
        # 2. Tokenize (PyVi)
        tokenized_text = ViTokenizer.tokenize(text)
        
        # 3. Filter vá»›i LIGHT_STOPWORDS - giá»¯ láº¡i nhiá»u context hÆ¡n
        tokens = tokenized_text.split()
        filtered_tokens = []
        
        for word in tokens:
            # Giá»¯ láº¡i náº¿u lÃ  critical keyword HOáº¶C khÃ´ng pháº£i light stopword
            if word in self.CRITICAL_KEYWORDS:
                filtered_tokens.append(word)  # LuÃ´n giá»¯ critical keywords
            elif word not in self.LIGHT_STOPWORDS and len(word) > 1:
                filtered_tokens.append(word)
        
        # 4. Má»Ÿ rá»™ng vá»›i synonyms (tÄƒng kháº£ nÄƒng matching)
        processed_text = ' '.join(filtered_tokens)
        expanded_text = self.expand_query(processed_text)
        
        return expanded_text

    def expand_query(self, text: str) -> str:
        """Má»Ÿ rá»™ng truy váº¥n báº±ng cÃ¡ch thÃªm tá»« Ä‘á»“ng nghÄ©a (Optimized)."""
        if not text: return ""
        
        expanded_words = []
        text_lower = text.lower()
        
        # 1. Má»Ÿ rá»™ng tá»« Ä‘Æ¡n
        words = text.split()
        for word in words:
            expanded_words.append(word)
            if word.lower() in self.SYNONYMS:
                expanded_words.extend(self.SYNONYMS[word.lower()])
        
        # 2. Má»Ÿ rá»™ng cá»¥m tá»« (DÃ¹ng Regex thay vÃ¬ Loop)
        if self.re_synonym_phrases:
            matches = self.re_synonym_phrases.findall(text_lower)
            for match in matches:
                # match lÃ  cá»¥m tá»« tÃ¬m tháº¥y (vÃ­ dá»¥ "xe hÆ¡i") -> láº¥y synonyms cá»§a nÃ³
                if match in self.SYNONYMS:
                    expanded_words.extend(self.SYNONYMS[match])

        return " ".join(expanded_words)

    def detect_negation(self, text: str) -> str:
        """PhÃ¡t hiá»‡n vÃ  xá»­ lÃ½ phá»§ Ä‘á»‹nh."""
        if not text: return ""
        tokens = text.split()
        processed = []
        negation_active = False
        
        for token in tokens:
            if token.lower() in self.NEGATION_WORDS:
                negation_active = True
                processed.append(token)
            elif negation_active:
                processed.append(f"NOT_{token}")
                negation_active = False
            else:
                processed.append(token)
        return " ".join(processed)

    def weighted_keyword_match(self, text: str) -> float:
        """TÃ­nh Ä‘iá»ƒm khá»›p tá»« khÃ³a quan trá»ng."""
        if not text: return 0.0
        score = 0.0
        text_lower = text.lower()
        for kw, weight in self.WEIGHTED_KEYWORDS.items():
            if kw in text_lower:
                score += weight
        return score

    def canonicalize_text(self, text: str) -> set:
        """Chuáº©n hÃ³a vÄƒn báº£n vá» dáº¡ng tá»« khÃ³a gá»‘c."""
        if not text: return set()
        
        # Gá»i preprocess_text ná»™i bá»™
        tokens = self.preprocess_text(text).split()
        canonical_tokens = set()
        
        for token in tokens:
            if token in self.REVERSE_SYNONYMS:
                canonical_tokens.add(self.REVERSE_SYNONYMS[token])
            else:
                canonical_tokens.add(token)
        return canonical_tokens

    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng Jaccard trÃªn táº­p tá»« Ä‘Ã£ chuáº©n hÃ³a."""
        if not text1 or not text2: return 0.0
        
        set1 = self.canonicalize_text(text1)
        set2 = self.canonicalize_text(text2)
        
        if not set1 and not set2: return 0.0
        
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        
        return len(intersection) / len(union) if union else 0.0

# =========================================================
# ðŸš€ MODULE LEVEL INTERFACE (BACKWARD COMPATIBILITY)
# =========================================================

# Khá»Ÿi táº¡o Singleton
preprocessor = TextPreprocessor()

# Expose cÃ¡c hÃ m Ä‘á»ƒ cÃ¡c module khÃ¡c import nhÆ° cÅ©
def preprocess_text(text: str) -> str:
    return preprocessor.preprocess_text(text)

def preprocess_for_knn(text: str) -> str:
    """ðŸ†• Preprocessing nháº¹ cho KNN - giá»¯ tá»« khÃ³a quan trá»ng."""
    return preprocessor.preprocess_for_knn(text)

def expand_query(text: str) -> str:
    return preprocessor.expand_query(text)

def detect_negation(text: str) -> str:
    return preprocessor.detect_negation(text)

def weighted_keyword_match(text: str) -> float:
    return preprocessor.weighted_keyword_match(text)

def calculate_jaccard_similarity(text1: str, text2: str) -> float:
    return preprocessor.calculate_jaccard_similarity(text1, text2)

def train_vectorizer(corpus):
    """Giá»¯ nguyÃªn hÃ m train_vectorizer vÃ¬ nÃ³ Ä‘á»™c láº­p."""
    vectorizer = TfidfVectorizer(
        max_features=800,
        ngram_range=(1, 2),
        min_df=1,
        sublinear_tf=True
    )
    vectorizer.fit(corpus)
    return vectorizer