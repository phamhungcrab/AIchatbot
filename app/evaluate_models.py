# -------------------------------
# üìä evaluate_models.py ‚Äî ƒê√°nh gi√° Model NB + KNN v·ªõi Thang ƒëo Confidence Hi·ªán ƒë·∫°i
# M·ª•c ƒë√≠ch: Test tr√™n b·ªô d·ªØ li·ªáu validation v√† t·∫°o b√°o c√°o t·ªïng h·ª£p k·∫øt qu·∫£
# -------------------------------

import numpy as np
import pandas as pd
import pickle
import os
import json
from datetime import datetime

# Import c√°c module n·ªôi b·ªô
from preprocess import preprocess_text, preprocess_for_knn, train_vectorizer
from nb_module import CustomMultinomialNB, predict_topic
from knn_module import CustomKNN, find_answer_knn
from datastore import get_all_qa
from confidence_utils import UnifiedCalibrator, NaiveBayesCalibrator, KNNCalibrator

# =========================================================
# üìÅ THI·∫æT L·∫¨P ƒê∆Ø·ªúNG D·∫™N
# =========================================================
BASE_DIR = os.path.dirname(__file__)
MODEL_DIR = os.path.join(BASE_DIR, '..', 'models')
DATA_DIR = os.path.join(BASE_DIR, '..', 'data')
RESULTS_DIR = os.path.join(BASE_DIR, '..', 'results')

# T·∫°o th∆∞ m·ª•c results n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(RESULTS_DIR, exist_ok=True)

# =========================================================
# üîß 1. LOAD MODELS V√Ä D·ªÆ LI·ªÜU
# =========================================================
def load_models():
    """Load c√°c model ƒë√£ train t·ª´ file .pkl"""
    vectorizer_path = os.path.join(MODEL_DIR, 'vectorizer.pkl')
    nb_model_path = os.path.join(MODEL_DIR, 'nb_model.pkl')
    knn_model_path = os.path.join(MODEL_DIR, 'knn_model.pkl')
    
    with open(vectorizer_path, 'rb') as f:
        vectorizer = pickle.load(f)
    with open(nb_model_path, 'rb') as f:
        nb_model = pickle.load(f)
    with open(knn_model_path, 'rb') as f:
        knn_model = pickle.load(f)
    
    print("‚úÖ ƒê√£ load: vectorizer, nb_model, knn_model")
    return vectorizer, nb_model, knn_model


def load_validation_data():
    """Load d·ªØ li·ªáu validation t·ª´ CSV"""
    valid_path = os.path.join(DATA_DIR, 'qa_valid.csv')
    df = pd.read_csv(valid_path)
    
    # Preprocess c√°c c√¢u h·ªèi
    df['clean_question'] = df['question'].apply(preprocess_text)
    
    print(f"üìä ƒê√£ load {len(df)} m·∫´u validation")
    return df


# =========================================================
# üìà 2. THANG ƒêO CONFIDENCE HI·ªÜN ƒê·∫†I
# =========================================================
def calculate_confidence_metrics(predictions, ground_truth, confidences):
    """
    T√≠nh to√°n c√°c thang ƒëo confidence hi·ªán ƒë·∫°i:
    - Accuracy
    - Average Confidence (Mean)
    - Calibration Error (ECE - Expected Calibration Error)
    - Confidence vs Accuracy Correlation
    - Coverage at Threshold
    """
    # 1. Basic Metrics
    predictions = np.array(predictions)
    ground_truth = np.array(ground_truth)
    confidences = np.array(confidences)
    
    correct = predictions == ground_truth
    accuracy = np.mean(correct)
    avg_confidence = np.mean(confidences)
    
    # 2. ECE (Expected Calibration Error) - ƒêo ƒë·ªô tin c·∫≠y c·ªßa confidence
    # Chia confidence th√†nh 10 bins, t√≠nh |accuracy - confidence| trung b√¨nh
    n_bins = 10
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    ece = 0.0
    
    for i in range(n_bins):
        in_bin = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])
        if np.sum(in_bin) > 0:
            bin_accuracy = np.mean(correct[in_bin])
            bin_confidence = np.mean(confidences[in_bin])
            bin_weight = np.sum(in_bin) / len(confidences)
            ece += bin_weight * abs(bin_accuracy - bin_confidence)
    
    # 3. Coverage at different thresholds
    thresholds = [0.3, 0.5, 0.7, 0.9]
    coverage = {}
    accuracy_at_threshold = {}
    
    for thresh in thresholds:
        mask = confidences >= thresh
        coverage[thresh] = np.mean(mask) * 100  # % samples above threshold
        if np.sum(mask) > 0:
            accuracy_at_threshold[thresh] = np.mean(correct[mask]) * 100
        else:
            accuracy_at_threshold[thresh] = 0.0
    
    return {
        'accuracy': accuracy * 100,                          # % ƒë√∫ng
        'average_confidence': avg_confidence * 100,          # % trung b√¨nh confidence
        'expected_calibration_error': ece * 100,             # ECE (%) - c√†ng th·∫•p c√†ng t·ªët
        'coverage_at_threshold': coverage,                   # % s·ªë m·∫´u >= threshold
        'accuracy_at_threshold': accuracy_at_threshold,      # Accuracy v·ªõi c√°c m·∫´u >= threshold
        'total_samples': len(predictions)
    }


# =========================================================
# üß™ 3. ƒê√ÅNH GI√Å NAIVE BAYES (TOPIC CLASSIFICATION)
# =========================================================
def evaluate_naive_bayes(nb_model, vectorizer, df, calibrator=None):
    """
    ƒê√°nh gi√° Naive Bayes tr√™n task ph√¢n lo·∫°i topic.
    
    üìå NB s·ª≠ d·ª•ng:
    - TF-IDF Vectorizer (unigram + bigram, max 800 features)
    - CustomMultinomialNB v·ªõi Laplace Smoothing (alpha=0.1)
    - P(topic|words) = P(topic) * ‚àèP(word|topic)
    
    üìå Confidence Calibration (Temperature Scaling):
    - raw_conf = max(softmax(log_proba))
    - calibrated = max(softmax(log_proba / temperature))
    - Temperature > 1 l√†m "m·ªÅm" distribution ‚Üí confidence th·ª±c t·∫ø h∆°n
    """
    print("\n" + "="*60)
    print("üî¨ ƒê√ÅNH GI√Å NAIVE BAYES (TOPIC CLASSIFICATION)")
    print("="*60)
    
    predictions = []
    raw_confidences = []
    calibrated_confidences = []
    details = []
    
    for idx, row in df.iterrows():
        clean_q = row['clean_question']
        true_topic = row['topic']
        
        # Predict v·ªõi raw confidence
        pred_topic, raw_conf = predict_topic(nb_model, vectorizer, clean_q)
        
        # Calibrate confidence
        if calibrator:
            # L·∫•y full probability distribution
            X = vectorizer.transform([clean_q])
            proba = nb_model.predict_proba(X)[0]
            calibrated_conf = calibrator.calibrate_nb(proba)
        else:
            calibrated_conf = raw_conf
        
        predictions.append(pred_topic)
        raw_confidences.append(raw_conf)
        calibrated_confidences.append(calibrated_conf)
        details.append({
            'question': row['question'][:50] + '...' if len(row['question']) > 50 else row['question'],
            'true_topic': true_topic,
            'predicted_topic': pred_topic,
            'raw_confidence': raw_conf,
            'calibrated_confidence': calibrated_conf,
            'correct': pred_topic == true_topic
        })
    
    # T√≠nh metrics cho C·∫¢ raw v√† calibrated
    raw_metrics = calculate_confidence_metrics(predictions, df['topic'].values, raw_confidences)
    calibrated_metrics = calculate_confidence_metrics(predictions, df['topic'].values, calibrated_confidences)
    
    return {
        'model': 'Naive Bayes (Topic Classification)',
        'technique': {
            'algorithm': 'Custom Multinomial Naive Bayes',
            'formula': 'P(topic|X) ‚àù P(topic) √ó ‚àè P(word_i|topic)',
            'smoothing': 'Laplace Smoothing (alpha=0.1)',
            'vectorizer': 'TF-IDF (800 features, unigram+bigram, sublinear_tf=True)',
            'preprocessing': [
                'Lowercase',
                'X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát & s·ªë',
                'PyVi Tokenizer (t√°ch t·ª´ ti·∫øng Vi·ªát)',
                'L·ªçc Vietnamese Stopwords'
            ]
        },
        'confidence_calibration': {
            'method': 'Temperature Scaling',
            'formula': 'P_calibrated(c|X) = exp(log P(c|X) / T) / Œ£ exp(log P(k|X) / T)',
            'temperature': calibrator.nb_calibrator.temperature if calibrator else 1.0,
            'interpretation': 'T > 1 l√†m m·ªÅm distribution ‚Üí confidence th·ª±c t·∫ø h∆°n'
        },
        'metrics': {
            'raw': raw_metrics,
            'calibrated': calibrated_metrics
        },
        'sample_results': details[:10]  # Top 10 m·∫´u
    }


# =========================================================
# üîç 4. ƒê√ÅNH GI√Å KNN (ANSWER RETRIEVAL)
# =========================================================
def evaluate_knn(knn_model, vectorizer, df, calibrator=None):
    """
    ƒê√°nh gi√° KNN tr√™n task t√¨m c√¢u tr·∫£ l·ªùi.
    
    üìå KNN s·ª≠ d·ª•ng:
    - Cosine Distance = 1 - Cosine Similarity
    - Raw Confidence = 1 - Distance = Cosine Similarity
    - K = 5 neighbors (m·∫∑c ƒë·ªãnh)
    
    üìå Confidence Calibration (Sigmoid Scaling):
    - raw_conf = cosine_similarity (th∆∞·ªùng 0.2-0.6 v·ªõi TF-IDF)
    - calibrated = sigmoid(k * (raw_conf - midpoint))
    - Chuy·ªÉn similarity v·ªÅ scale [0,1] h·ª£p l√Ω h∆°n
    """
    print("\n" + "="*60)
    print("üîç ƒê√ÅNH GI√Å KNN (ANSWER RETRIEVAL)")
    print("="*60)
    
    predictions = []
    raw_confidences = []
    calibrated_confidences = []
    exact_matches = []
    details = []
    
    for idx, row in df.iterrows():
        # D√πng preprocess_for_knn thay v√¨ preprocess_text
        clean_q = preprocess_for_knn(row['question'])
        true_answer = row['answer']
        
        # T√¨m c√¢u tr·∫£ l·ªùi b·∫±ng KNN
        pred_answer, raw_conf, matched_q, topic, top_k = find_answer_knn(
            knn_model, vectorizer, clean_q, k=3
        )
        
        # Calibrate confidence
        if calibrator:
            calibrated_conf = calibrator.calibrate_knn(raw_conf)
        else:
            calibrated_conf = raw_conf
        
        # So s√°nh exact match
        is_exact = (pred_answer == true_answer)
        
        predictions.append(pred_answer)
        raw_confidences.append(raw_conf)
        calibrated_confidences.append(calibrated_conf)
        exact_matches.append(is_exact)
        details.append({
            'question': row['question'][:50] + '...' if len(row['question']) > 50 else row['question'],
            'true_answer': true_answer[:50] + '...' if len(true_answer) > 50 else true_answer,
            'predicted_answer': pred_answer[:50] + '...' if pred_answer and len(pred_answer) > 50 else pred_answer,
            'matched_question': matched_q[:50] + '...' if matched_q and len(matched_q) > 50 else matched_q,
            'raw_confidence': raw_conf,
            'calibrated_confidence': calibrated_conf,
            'exact_match': is_exact,
            'cosine_distance': round(1 - raw_conf, 4)
        })
    
    # T√≠nh metrics cho C·∫¢ raw v√† calibrated
    exact_matches_arr = np.array(exact_matches)
    raw_confidences_arr = np.array(raw_confidences)
    calibrated_confidences_arr = np.array(calibrated_confidences)
    
    def compute_knn_metrics(confidences, exact_matches):
        thresholds = [0.3, 0.5, 0.7, 0.9]
        coverage = {}
        accuracy_at_threshold = {}
        
        for thresh in thresholds:
            mask = confidences >= thresh
            coverage[thresh] = np.mean(mask) * 100
            if np.sum(mask) > 0:
                accuracy_at_threshold[thresh] = np.mean(exact_matches[mask]) * 100
            else:
                accuracy_at_threshold[thresh] = 0.0
        
        return {
            'exact_match_accuracy': np.mean(exact_matches) * 100,
            'average_confidence': np.mean(confidences) * 100,
            'coverage_at_threshold': coverage,
            'accuracy_at_threshold': accuracy_at_threshold,
            'total_samples': len(confidences)
        }
    
    raw_metrics = compute_knn_metrics(raw_confidences_arr, exact_matches_arr)
    calibrated_metrics = compute_knn_metrics(calibrated_confidences_arr, exact_matches_arr)
    
    return {
        'model': 'KNN (Answer Retrieval)',
        'technique': {
            'algorithm': 'Custom K-Nearest Neighbors',
            'distance_metric': 'Cosine Distance = 1 - (A¬∑B)/(||A||√ó||B||)',
            'raw_confidence_formula': 'Raw Confidence = 1 - Cosine Distance = Cosine Similarity',
            'k_neighbors': 5,
            'vectorizer': 'TF-IDF (800 features, unigram+bigram, sublinear_tf=True)',
            'preprocessing': [
                'Lowercase',
                'X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát (KH√îNG x√≥a s·ªë)',
                'PyVi Tokenizer (t√°ch t·ª´ ti·∫øng Vi·ªát)',
                'LIGHT_STOPWORDS (gi·ªØ t·ª´ kh√≥a quan tr·ªçng)',
                'Synonym Expansion (m·ªü r·ªông v·ªõi t·ª´ ƒë·ªìng nghƒ©a)'
            ]
        },
        'confidence_calibration': {
            'method': 'Sigmoid Scaling',
            'formula': 'calibrated = 1 / (1 + exp(-k √ó (similarity - midpoint)))',
            'k': calibrator.knn_calibrator.k if calibrator else 10.0,
            'midpoint': calibrator.knn_calibrator.midpoint if calibrator else 0.4,
            'interpretation': 'Chuy·ªÉn similarity t·ª´ [0.2-0.6] v·ªÅ [0-1] h·ª£p l√Ω h∆°n'
        },
        'metrics': {
            'raw': raw_metrics,
            'calibrated': calibrated_metrics
        },
        'sample_results': details[:10]
    }


# =========================================================
# üìù 5. T·∫†O B√ÅO C√ÅO T·ªîNG H·ª¢P
# =========================================================
def generate_report(nb_results, knn_results):
    """T·∫°o b√°o c√°o t·ªïng h·ª£p d·∫°ng dictionary v√† l∆∞u ra file"""
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    report = {
        'evaluation_timestamp': timestamp,
        'summary': {
            'description': 'ƒê√°nh gi√° h·ªá th·ªëng AI Chatbot v·ªõi 2 model: Naive Bayes (ph√¢n lo·∫°i topic) v√† KNN (t√¨m c√¢u tr·∫£ l·ªùi)',
            'data_source': 'qa_valid.csv',
            'total_test_samples': nb_results['metrics']['calibrated']['total_samples'],
        },
        'preprocessing_pipeline': {
            'description': 'Quy tr√¨nh ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát',
            'steps': [
                '1. Lowercase: Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng',
                '2. Special Char Removal: X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát b·∫±ng Regex r"[^\\w\\s]"',
                '3. Number Removal: X√≥a s·ªë b·∫±ng Regex r"\\d+"',
                '4. PyVi Tokenizer: T√°ch t·ª´ ti·∫øng Vi·ªát (ViTokenizer.tokenize)',
                '5. Stopword Removal: L·ªçc c√°c t·ª´ d·ª´ng ti·∫øng Vi·ªát (52 t·ª´)',
            ],
            'vectorization': {
                'method': 'TF-IDF (Term Frequency - Inverse Document Frequency)',
                'params': {
                    'max_features': 800,
                    'ngram_range': '(1, 2) - unigram + bigram',
                    'min_df': 1,
                    'sublinear_tf': True,
                    'formula': 'TF-IDF(t,d) = (1 + log(tf)) √ó log(N/df)'
                }
            }
        },
        'models': {
            'naive_bayes': nb_results,
            'knn': knn_results
        },
        'confidence_metrics_explanation': {
            'accuracy': 'T·ª∑ l·ªá % d·ª± ƒëo√°n ƒë√∫ng',
            'average_confidence': 'Gi√° tr·ªã confidence trung b√¨nh c·ªßa model',
            'expected_calibration_error': 'ECE - ƒëo m·ª©c ƒë·ªô tin c·∫≠y c·ªßa confidence (c√†ng th·∫•p c√†ng t·ªët, <5% l√† t·ªët)',
            'coverage_at_threshold': 'T·ª∑ l·ªá % m·∫´u c√≥ confidence >= threshold',
            'accuracy_at_threshold': 'Accuracy ch·ªâ t√≠nh tr√™n c√°c m·∫´u c√≥ confidence >= threshold'
        }
    }
    
    return report


def save_report(report, format='both'):
    """L∆∞u b√°o c√°o ra file JSON v√†/ho·∫∑c Markdown"""
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    if format in ['json', 'both']:
        json_path = os.path.join(RESULTS_DIR, f'evaluation_report_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"üìÅ ƒê√£ l∆∞u: {json_path}")
    
    if format in ['md', 'both']:
        md_path = os.path.join(RESULTS_DIR, f'evaluation_report_{timestamp}.md')
        md_content = generate_markdown_report(report)
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"üìÅ ƒê√£ l∆∞u: {md_path}")
    
    return json_path if format in ['json', 'both'] else md_path


def generate_markdown_report(report):
    """T·∫°o b√°o c√°o d·∫°ng Markdown v·ªõi Raw vs Calibrated comparison"""
    
    nb = report['models']['naive_bayes']
    knn = report['models']['knn']
    
    md = f"""# üìä B√ÅO C√ÅO ƒê√ÅNH GI√Å AI CHATBOT (CALIBRATED CONFIDENCE)

> **Th·ªùi gian ƒë√°nh gi√°:** {report['evaluation_timestamp']}  
> **T·ªïng s·ªë m·∫´u test:** {report['summary']['total_test_samples']}

---

## üßπ 1. QUY TR√åNH TI·ªÄN X·ª¨ L√ù (Preprocessing)

{chr(10).join('- ' + step for step in report['preprocessing_pipeline']['steps'])}

### TF-IDF Vectorization
| Tham s·ªë | Gi√° tr·ªã |
|---------|---------|
| max_features | {report['preprocessing_pipeline']['vectorization']['params']['max_features']} |
| ngram_range | {report['preprocessing_pipeline']['vectorization']['params']['ngram_range']} |
| sublinear_tf | {report['preprocessing_pipeline']['vectorization']['params']['sublinear_tf']} |
| **C√¥ng th·ª©c** | `{report['preprocessing_pipeline']['vectorization']['params']['formula']}` |

---

## ü§ñ 2. NAIVE BAYES (Ph√¢n lo·∫°i Topic)

### K·ªπ thu·∫≠t s·ª≠ d·ª•ng
- **Thu·∫≠t to√°n:** {nb['technique']['algorithm']}
- **C√¥ng th·ª©c:** `{nb['technique']['formula']}`
- **Smoothing:** {nb['technique']['smoothing']}

### Confidence Calibration (Temperature Scaling)

| Tham s·ªë | Gi√° tr·ªã |
|---------|---------|
| **Method** | {nb['confidence_calibration']['method']} |
| **Formula** | `{nb['confidence_calibration']['formula']}` |
| **Temperature** | {nb['confidence_calibration']['temperature']} |
| **√ù nghƒ©a** | {nb['confidence_calibration']['interpretation']} |

### K·∫øt qu·∫£ (RAW vs CALIBRATED)

| Metric | Raw | Calibrated |
|--------|-----|------------|
| **Accuracy** | {nb['metrics']['raw']['accuracy']:.2f}% | (kh√¥ng ƒë·ªïi) |
| **Avg Confidence** | {nb['metrics']['raw']['average_confidence']:.2f}% | **{nb['metrics']['calibrated']['average_confidence']:.2f}%** |
| **ECE** | {nb['metrics']['raw']['expected_calibration_error']:.2f}% | **{nb['metrics']['calibrated']['expected_calibration_error']:.2f}%** |

### Coverage & Accuracy theo Threshold (Calibrated)

| Threshold | Coverage | Accuracy |
|-----------|----------|----------|
| ‚â• 0.3 | {nb['metrics']['calibrated']['coverage_at_threshold'].get(0.3, 0):.1f}% | {nb['metrics']['calibrated']['accuracy_at_threshold'].get(0.3, 0):.1f}% |
| ‚â• 0.5 | {nb['metrics']['calibrated']['coverage_at_threshold'].get(0.5, 0):.1f}% | {nb['metrics']['calibrated']['accuracy_at_threshold'].get(0.5, 0):.1f}% |
| ‚â• 0.7 | {nb['metrics']['calibrated']['coverage_at_threshold'].get(0.7, 0):.1f}% | {nb['metrics']['calibrated']['accuracy_at_threshold'].get(0.7, 0):.1f}% |
| ‚â• 0.9 | {nb['metrics']['calibrated']['coverage_at_threshold'].get(0.9, 0):.1f}% | {nb['metrics']['calibrated']['accuracy_at_threshold'].get(0.9, 0):.1f}% |

---

## üîç 3. KNN (T√¨m c√¢u tr·∫£ l·ªùi)

### K·ªπ thu·∫≠t s·ª≠ d·ª•ng
- **Thu·∫≠t to√°n:** {knn['technique']['algorithm']}
- **Distance Metric:** `{knn['technique']['distance_metric']}`
- **Raw Confidence:** `{knn['technique']['raw_confidence_formula']}`
- **K neighbors:** {knn['technique']['k_neighbors']}

### Preprocessing cho KNN (Kh√°c v·ªõi NB)
{chr(10).join('- ' + step for step in knn['technique']['preprocessing'])}

### Confidence Calibration (Sigmoid Scaling)

| Tham s·ªë | Gi√° tr·ªã |
|---------|---------|
| **Method** | {knn['confidence_calibration']['method']} |
| **Formula** | `{knn['confidence_calibration']['formula']}` |
| **k (steepness)** | {knn['confidence_calibration']['k']} |
| **midpoint** | {knn['confidence_calibration']['midpoint']} |
| **√ù nghƒ©a** | {knn['confidence_calibration']['interpretation']} |

### K·∫øt qu·∫£ (RAW vs CALIBRATED)

| Metric | Raw | Calibrated |
|--------|-----|------------|
| **Exact Match** | {knn['metrics']['raw']['exact_match_accuracy']:.2f}% | (kh√¥ng ƒë·ªïi) |
| **Avg Confidence** | {knn['metrics']['raw']['average_confidence']:.2f}% | **{knn['metrics']['calibrated']['average_confidence']:.2f}%** |

### Coverage & Accuracy theo Threshold (Calibrated)

| Threshold | Coverage | Accuracy |
|-----------|----------|----------|
| ‚â• 0.3 | {knn['metrics']['calibrated']['coverage_at_threshold'].get(0.3, 0):.1f}% | {knn['metrics']['calibrated']['accuracy_at_threshold'].get(0.3, 0):.1f}% |
| ‚â• 0.5 | {knn['metrics']['calibrated']['coverage_at_threshold'].get(0.5, 0):.1f}% | {knn['metrics']['calibrated']['accuracy_at_threshold'].get(0.5, 0):.1f}% |
| ‚â• 0.7 | {knn['metrics']['calibrated']['coverage_at_threshold'].get(0.7, 0):.1f}% | {knn['metrics']['calibrated']['accuracy_at_threshold'].get(0.7, 0):.1f}% |
| ‚â• 0.9 | {knn['metrics']['calibrated']['coverage_at_threshold'].get(0.9, 0):.1f}% | {knn['metrics']['calibrated']['accuracy_at_threshold'].get(0.9, 0):.1f}% |

---

## üìñ 4. GI·∫¢I TH√çCH CALIBRATION

### T·∫°i sao c·∫ßn Calibration?
- **NB**: Confidence th∆∞·ªùng CAO qu√° (64% vs accuracy 47%) ‚Üí Temperature Scaling l√†m "m·ªÅm"
- **KNN**: Cosine similarity th∆∞·ªùng TH·∫§P v·ªõi TF-IDF (0.2-0.6) ‚Üí Sigmoid Scaling ƒë∆∞a v·ªÅ [0,1] h·ª£p l√Ω

### Metrics sau Calibration
| Metric | √ù nghƒ©a |
|--------|---------|
| **Accuracy** | T·ª∑ l·ªá % d·ª± ƒëo√°n ƒë√∫ng (kh√¥ng ƒë·ªïi) |
| **Avg Confidence** | Gi√° tr·ªã confidence trung b√¨nh ƒê√É CALIBRATE |
| **ECE** | Expected Calibration Error - l√Ω t∆∞·ªüng n√™n ‚âà 0% |
| **Coverage** | % m·∫´u c√≥ confidence ‚â• threshold |

---

## üß† 5. C√îNG TH·ª®C CHI TI·∫æT

### NB Temperature Scaling
```
log_proba = log P(c) + Œ£ log P(word_i|c)
calibrated = softmax(log_proba / Temperature)
confidence = max(calibrated)
```

### KNN Sigmoid Scaling
```
raw_similarity = 1 - cosine_distance
calibrated = 1 / (1 + exp(-k √ó (raw_similarity - midpoint)))
```

---

*B√°o c√°o ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi evaluate_models.py v·ªõi calibrated confidence*
"""
    return md


# =========================================================
# üöÄ MAIN
# =========================================================
if __name__ == "__main__":
    print("\n" + "="*60)
    print("üöÄ B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å MODELS V·ªöI CALIBRATED CONFIDENCE")
    print("="*60)
    
    # 1. Load models v√† d·ªØ li·ªáu
    vectorizer, nb_model, knn_model = load_models()
    df_valid = load_validation_data()
    
    # 2. Kh·ªüi t·∫°o Calibrator
    # - NB: Temperature=1.5 (l√†m m·ªÅm confidence)
    # - KNN: k=10, midpoint=0.4 (sigmoid scaling)
    calibrator = UnifiedCalibrator(
        nb_temperature=1.5,
        knn_k=10.0,
        knn_midpoint=0.4
    )
    print(f"üìê Calibrator: NB(T={calibrator.nb_calibrator.temperature}), KNN(k={calibrator.knn_calibrator.k}, mid={calibrator.knn_calibrator.midpoint})")
    
    # 3. ƒê√°nh gi√° t·ª´ng model V·ªöI calibration
    nb_results = evaluate_naive_bayes(nb_model, vectorizer, df_valid, calibrator)
    knn_results = evaluate_knn(knn_model, vectorizer, df_valid, calibrator)
    
    # 4. In k·∫øt qu·∫£ t√≥m t·∫Øt - SO S√ÅNH RAW vs CALIBRATED
    print("\n" + "="*60)
    print("üìä K·∫æT QU·∫¢ T·ªîNG H·ª¢P (RAW vs CALIBRATED)")
    print("="*60)
    
    print(f"\nü§ñ NAIVE BAYES (Topic Classification):")
    print(f"   ‚Ä¢ Accuracy: {nb_results['metrics']['raw']['accuracy']:.2f}%")
    print(f"   ‚Ä¢ [RAW] Avg Confidence: {nb_results['metrics']['raw']['average_confidence']:.2f}%")
    print(f"   ‚Ä¢ [CALIBRATED] Avg Confidence: {nb_results['metrics']['calibrated']['average_confidence']:.2f}%")
    print(f"   ‚Ä¢ [RAW] ECE: {nb_results['metrics']['raw']['expected_calibration_error']:.2f}%")
    print(f"   ‚Ä¢ [CALIBRATED] ECE: {nb_results['metrics']['calibrated']['expected_calibration_error']:.2f}%")
    
    print(f"\nüîç KNN (Answer Retrieval):")
    print(f"   ‚Ä¢ Exact Match: {knn_results['metrics']['raw']['exact_match_accuracy']:.2f}%")
    print(f"   ‚Ä¢ [RAW] Avg Confidence: {knn_results['metrics']['raw']['average_confidence']:.2f}%")
    print(f"   ‚Ä¢ [CALIBRATED] Avg Confidence: {knn_results['metrics']['calibrated']['average_confidence']:.2f}%")
    
    # 5. T·∫°o v√† l∆∞u b√°o c√°o
    report = generate_report(nb_results, knn_results)
    saved_path = save_report(report, format='both')
    
    print("\n" + "="*60)
    print("‚úÖ ƒê√ÅNH GI√Å HO√ÄN T·∫§T!")
    print("="*60)

