# -------------------------------
# üß† train_models.py ‚Äî Hu·∫•n luy·ªán to√†n b·ªô m√¥ h√¨nh cho Chatbot
# Ch·ª©c nƒÉng:
#   - ƒê·ªçc d·ªØ li·ªáu Q&A t·ª´ c∆° s·ªü d·ªØ li·ªáu SQLite (knowledge.db)
#   - Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (chu·∫©n h√≥a ng√¥n ng·ªØ)
#   - Hu·∫•n luy·ªán TF-IDF vectorizer, m√¥ h√¨nh Na√Øve Bayes, v√† KNN
#   - L∆∞u c√°c m√¥ h√¨nh ra th∆∞ m·ª•c "models/"
# -------------------------------
import pandas as pd
import pickle
import nltk
import os
import ssl # <--- Fix l·ªói SSL

# --- ƒêO·∫†N CODE FIX L·ªñI SSL (B·∫°n v·ª´a th√™m) ---
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# --- C√ÅC IMPORT QUAN TR·ªåNG (Ki·ªÉm tra kƒ© ƒëo·∫°n n√†y) ---
# üëá B·∫°n ƒëang thi·∫øu ho·∫∑c b·ªã l·ªói d√≤ng n√†y:
from datastore import get_all_qa                  
from preprocess import preprocess_text, train_vectorizer
from nb_module import train_naive_bayes
# -------------------------------
# üì¶ T·∫¢I D·ªÆ LI·ªÜU H·ªñ TR·ª¢ T·ª™ NLTK (l·∫ßn ƒë·∫ßu ti√™n ch·∫°y)
# -------------------------------
# C√°c g√≥i n√†y gi√∫p tokenization, stopwords filtering v√† lemmatization ho·∫°t ƒë·ªông ch√≠nh x√°c.
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')   # (b·ªï sung ƒë·ªÉ h·ªó tr·ª£ m·ªôt s·ªë tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát trong NLTK)

# -------------------------------
# üìÅ Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c l∆∞u m√¥ h√¨nh
# -------------------------------
BASE_DIR = os.path.dirname(__file__)               # ‚Üí th∆∞ m·ª•c hi·ªán t·∫°i ("app/")
MODEL_DIR = os.path.join(BASE_DIR, '..', 'models') # ‚Üí l√πi 1 c·∫•p ƒë·∫øn th∆∞ m·ª•c "models/"
# os.makedirs(MODEL_DIR, exist_ok=True)            # B·∫≠t l·∫°i n·∫øu c·∫ßn t·ª± ƒë·ªông t·∫°o th∆∞ m·ª•c

# =========================================================
# üöÄ H√ÄM CH√çNH: HU·∫§N LUY·ªÜN TO√ÄN B·ªò M√î H√åNH CHATBOT
# =========================================================
def train_all_models():
    """
    ‚úÖ M·ª•c ƒë√≠ch:
        - ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu Q&A t·ª´ knowledge.db
        - Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
        - Hu·∫•n luy·ªán TF-IDF, Na√Øve Bayes, v√† KNN
        - L∆∞u m√¥ h√¨nh ra th∆∞ m·ª•c models/

    üîÅ K·∫øt qu·∫£:
        models/
        ‚îú‚îÄ‚îÄ vectorizer.pkl
        ‚îú‚îÄ‚îÄ nb_model.pkl
        ‚îî‚îÄ‚îÄ knn_model.pkl
    """

    # ------------------------------------
    # 1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu t·ª´ database
    # ------------------------------------
    print('üìö ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ knowledge.db...')
    df = get_all_qa()  # Tr·∫£ v·ªÅ DataFrame g·ªìm [question, answer, topic]

    # Ki·ªÉm tra d·ªØ li·ªáu c√≥ tr·ªëng kh√¥ng
    if df.empty:
        print('‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong c∆° s·ªü d·ªØ li·ªáu! Ki·ªÉm tra knowledge.db.')
        return

    # ------------------------------------
    # 2Ô∏è‚É£ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu vƒÉn b·∫£n
    # ------------------------------------
    # G·ªçi h√†m preprocess_text() cho t·ª´ng c√¢u h·ªèi
    #   - Chuy·ªÉn ch·ªØ th∆∞·ªùng
    #   - X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát, s·ªë, stopwords
    #   - Tokenize l·∫°i th√†nh vƒÉn b·∫£n s·∫°ch
    print('üßπ ƒêang ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu...')
    df['clean_text'] = df['question'].apply(preprocess_text)

    # ------------------------------------
    # 3Ô∏è‚É£ Hu·∫•n luy·ªán TF-IDF vectorizer
    # ------------------------------------
    # TF-IDF s·∫Ω chuy·ªÉn t·ª´ng c√¢u h·ªèi th√†nh vector s·ªë h·ªçc (ƒë·∫∑c tr∆∞ng)
    print('‚öôÔ∏è ƒêang hu·∫•n luy·ªán TF-IDF vectorizer...')
    vectorizer = train_vectorizer(df['clean_text'])

    # L∆∞u TF-IDF ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ d√πng l·∫°i khi d·ª± ƒëo√°n
    with open(os.path.join(MODEL_DIR, 'vectorizer.pkl'), 'wb') as f:
        pickle.dump(vectorizer, f)

    # ------------------------------------
    # 4Ô∏è‚É£ Hu·∫•n luy·ªán m√¥ h√¨nh Na√Øve Bayes
    # ------------------------------------
    # M√¥ h√¨nh n√†y h·ªçc c√°ch ph√¢n lo·∫°i c√¢u h·ªèi v√†o c√°c ch·ªß ƒë·ªÅ (topics)
    print('üß† ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Na√Øve Bayes...')
    nb_model = train_naive_bayes(vectorizer, df['clean_text'], df['topic'])

    # ------------------------------------
    # 5Ô∏è‚É£ Hu·∫•n luy·ªán m√¥ h√¨nh KNN
    # ------------------------------------
    # M√¥ h√¨nh n√†y d√πng ƒë·ªÉ t√¨m c√¢u h·ªèi t∆∞∆°ng t·ª± nh·∫•t ‚Üí ch·ªçn c√¢u tr·∫£ l·ªùi g·∫ßn nh·∫•t
    # ------------------------------------
    # 6Ô∏è‚É£ K·∫øt th√∫c qu√° tr√¨nh hu·∫•n luy·ªán
    # ------------------------------------
    print('‚úÖ Ho√†n t·∫•t hu·∫•n luy·ªán!')
    print('üì¶ C√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c: models/')
    print('   ‚îú‚îÄ‚îÄ vectorizer.pkl')
    print('   ‚îú‚îÄ‚îÄ nb_model.pkl')


# =========================================================
# ‚ñ∂Ô∏è CH·∫†Y TR·ª∞C TI·∫æP FILE
# =========================================================
if __name__ == '__main__':
    # Khi ch·∫°y file b·∫±ng l·ªánh:
    #   python app/train_models.py
    # ‚Üí To√†n b·ªô quy tr√¨nh hu·∫•n luy·ªán s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán
    train_all_models()
