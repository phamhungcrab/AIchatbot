# -------------------------------
# üß† Chatbot h·ªçc t·∫≠p cho m√¥n Nh·∫≠p m√¥n Tr√≠ tu·ªá Nh√¢n t·∫°o (IT3160)
# File n√†y l√† "main.py" ‚Äì file ch√≠nh kh·ªüi ch·∫°y ·ª©ng d·ª•ng Flask
# -------------------------------

# üì¶ Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
from flask import Flask, render_template, request, redirect, url_for  # Flask framework ƒë·ªÉ x√¢y web app
import pandas as pd              # X·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng
import pickle                    # ƒê·ªçc file model ƒë√£ l∆∞u (Naive Bayes, KNN, vectorizer)
from preprocess import preprocess_text       # H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (lo·∫°i b·ªè stopword, k√Ω t·ª± ƒë·∫∑c bi·ªát...)
from nb_module import predict_topic          # H√†m d·ª± ƒëo√°n ch·ªß ƒë·ªÅ b·∫±ng m√¥ h√¨nh Na√Øve Bayes
from find_answer import find_best_answer      # H√†m t√¨m c√¢u tr·∫£ l·ªùi g·∫ßn nh·∫•t b·∫±ng KNN
from datastore import get_all_qa, get_qa_by_topic  # C√°c h√†m truy xu·∫•t d·ªØ li·ªáu Q&A t·ª´ SQLite

import os                       # Th∆∞ vi·ªán thao t√°c v·ªõi ƒë∆∞·ªùng d·∫´n file/th∆∞ m·ª•c
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# -------------------------------
# ‚öôÔ∏è Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n cho Flask
# -------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, "..")

# -------------------------------
# üìÇ N·∫°p m√¥ h√¨nh Generative (Fallback)
# -------------------------------
MODEL_PATH = os.path.join(ROOT_DIR, 'models', 'my_generative_bot')
print("‚è≥ ƒêang t·∫£i model Generative Bot (Fallback)...")
try:
    gen_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    gen_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)
    print("‚úÖ Generative Model ƒë√£ s·∫µn s√†ng!")
except Exception as e:
    print(f"‚ùå L·ªói load Generative Model: {e}")
    gen_model = None

def generate_answer_local(question):
    if not gen_model:
        return None
    try:
        input_text = f"question: {question}"
        input_ids = gen_tokenizer(input_text, return_tensors="pt").input_ids
        outputs = gen_model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)
        return gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        print(f"‚ùå L·ªói sinh c√¢u tr·∫£ l·ªùi: {e}")
        return None

# -------------------------------
# üöÄ Kh·ªüi t·∫°o ·ª©ng d·ª•ng Flask
# -------------------------------
app = Flask(
    __name__,
    template_folder=os.path.join(ROOT_DIR, "templates"),  # Th∆∞ m·ª•c ch·ª©a file .html (Jinja2 templates)
    static_folder=os.path.join(ROOT_DIR, "static")        # Th∆∞ m·ª•c ch·ª©a CSS, JS, ·∫£nh, favicon, v.v.
)

# -------------------------------
# üìÇ N·∫°p m√¥ h√¨nh h·ªçc m√°y ƒë√£ hu·∫•n luy·ªán s·∫µn
# -------------------------------

# vectorizer.pkl: m√¥ h√¨nh chuy·ªÉn vƒÉn b·∫£n th√†nh vector s·ªë (TF-IDF, Bag-of-Words, v.v.)
with open('models/vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

# nb_model.pkl: m√¥ h√¨nh Na√Øve Bayes ‚Üí d√πng ƒë·ªÉ d·ª± ƒëo√°n ch·ªß ƒë·ªÅ (topic)
with open('models/nb_model.pkl', 'rb') as f:
    nb_model = pickle.load(f)

# knn_model.pkl: KH√îNG S·ª¨ D·ª§NG (ƒë√£ chuy·ªÉn sang Cosine Similarity)
# with open('models/knn_model.pkl', 'rb') as f:
#     knn_model = pickle.load(f)

# -------------------------------
# üí¨ Bi·∫øn l∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i
# -------------------------------
# L∆∞u t·∫°m trong b·ªô nh·ªõ RAM (d·∫°ng list), s·∫Ω m·∫•t khi reload server
chat_history = []


# -------------------------------
# üåê ROUTE CH√çNH: Trang Chatbot
# -------------------------------
@app.route('/', methods=['GET', 'POST'])
def chatbot():
    global chat_history
    
    if request.method == 'POST':
        user_message = request.form['user_message']
        
        if user_message.strip():
            # B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω
            processed = preprocess_text(user_message)
            
            # B∆∞·ªõc 2: D·ª± ƒëo√°n topic
            topic, topic_confidence = predict_topic(nb_model, vectorizer, processed)
            
            # B∆∞·ªõc 3: L·∫•y c√¢u h·ªèi trong topic
            df_topic = get_qa_by_topic(topic)
            
            # B∆∞·ªõc 4: T√¨m best match v·ªõi threshold
            result = find_best_answer(
                vectorizer, 
                processed,  # ‚úÖ D√πng processed thay v√¨ user_message
                df_topic, 
                threshold=0.5  # ‚úÖ Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu
            )
            
            answer, question_similarity, matched_question = result
            
            # ‚úÖ T√≠nh final confidence
            # ‚úÖ T√≠nh final confidence
            if answer is None:
                # Case 1: Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi n√†o trong DB (do threshold c·ªßa find_best_answer)
                final_confidence = 0.0
                print("DEBUG: answer is None -> final_confidence = 0.0")
            else:
                # Case 2: T√¨m th·∫•y, nh∆∞ng c·∫ßn ki·ªÉm tra ƒë·ªô tin c·∫≠y t·ªïng h·ª£p
                # ‚úÖ C√¥ng th·ª©c m·ªõi (NB-Centric): Naive Bayes quy·∫øt ƒë·ªãnh ch√≠nh
                final_confidence = (
                    0.60 * topic_confidence +      # 60% - Naive Bayes quy·∫øt ƒë·ªãnh ch√≠nh
                    0.30 * question_similarity +   # 30% - H·ªó tr·ª£ t√¨m c√¢u tr·∫£ l·ªùi c·ª• th·ªÉ
                    0.10 * 0.8                     # 10% - Y·∫øu t·ªë kh√°c
                )
                print(f"DEBUG: Found answer. final_confidence = {final_confidence}")

            # ---------------------------------------------------------
            # ü§ñ QUY·∫æT ƒê·ªäNH: D√πng c√¢u tr·∫£ l·ªùi t·ª´ DB hay g·ªçi AI?
            # ---------------------------------------------------------
            
            # Ng∆∞·ª°ng ƒë·ªÉ ch·∫•p nh·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ DB (v√≠ d·ª•: 0.55)
            CONFIDENCE_THRESHOLD = 0.55

            if final_confidence >= CONFIDENCE_THRESHOLD:
                # --- ƒê·ª¶ ƒê·ªò TIN C·∫¨Y ---
                print("DEBUG: Confidence >= Threshold. Using DB answer.")
                if final_confidence >= 0.80:
                    pass  # R·∫•t tin c·∫≠y (>= 80%), kh√¥ng c·∫ßn disclaimer
                elif final_confidence >= 0.65:
                    answer += "\n\nüí° N·∫øu c√¢u tr·∫£ l·ªùi ch∆∞a ch√≠nh x√°c, h√£y h·ªèi chi ti·∫øt h∆°n."
                elif final_confidence >= 0.55:
                    answer += "\n\n‚ö†Ô∏è T√¥i kh√¥ng ho√†n to√†n ch·∫Øc ch·∫Øn. B·∫°n c√≥ th·ªÉ h·ªèi theo c√°ch kh√°c?"
            else:
                # --- KH√îNG ƒê·ª¶ ƒê·ªò TIN C·∫¨Y (ho·∫∑c kh√¥ng t√¨m th·∫•y) -> D√ôNG GENERATIVE MODEL ---
                print(f"DEBUG: Confidence th·∫•p ({final_confidence:.2f}). Chuy·ªÉn sang Generative Model...")
                
                gen_answer = generate_answer_local(user_message)
                
                if gen_answer:
                    answer = gen_answer + "\n\nü§ñ (C√¢u tr·∫£ l·ªùi t·ª± ƒë·ªông t·ª´ AI)"
                    final_confidence = 0.9 # Gi·∫£ ƒë·ªãnh confidence cao cho AI
                    topic = "Generative AI"
                else:
                    answer = "Xin l·ªói, t√¥i ch∆∞a ƒë∆∞·ª£c h·ªçc v·ªÅ v·∫•n ƒë·ªÅ n√†y v√† AI c≈©ng kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c."
            
            # ‚úÖ L∆∞u k√®m confidence (optional - ƒë·ªÉ debug/analysis)
            chat_history.append({
                "user": user_message,
                "bot": answer,
                "confidence": round(final_confidence, 3),
                "topic": topic,
                "topic_conf": round(topic_confidence, 3),
                "question_sim": round(question_similarity, 3) if question_similarity else 0.0
            })
        
        return redirect(url_for('chatbot'))
    
    return render_template('index.html', chat_history=chat_history)


# -------------------------------
# üßπ ROUTE PH·ª§: X√≥a to√†n b·ªô l·ªãch s·ª≠ chat
# -------------------------------
@app.route('/clear', methods=['POST'])
def clear_history():
    """
    Khi ng∆∞·ªùi d√πng b·∫•m n√∫t 'X√≥a l·ªãch s·ª≠' ‚Üí reset l·∫°i danh s√°ch chat_history
    """
    global chat_history
    chat_history = []  # L√†m tr·ªëng danh s√°ch h·ªôi tho·∫°i
    return redirect(url_for('chatbot'))  # Quay l·∫°i trang chatbot ch√≠nh


# -------------------------------
# ‚ñ∂Ô∏è Ch·∫°y Flask app
# -------------------------------
if __name__ == '__main__':
    # debug=True gi√∫p auto reload khi thay ƒë·ªïi code v√† hi·ªÉn th·ªã log l·ªói chi ti·∫øt
    app.run(debug=True, host='0.0.0.0', port=5002) # Ch·∫°y port 5002 ƒë·ªÉ tr√°nh AirPlay (port 5000)
